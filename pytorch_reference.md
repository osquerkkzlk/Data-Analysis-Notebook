# pytorch



## 1、张量运算

[参考该链接](#1、张量)

------

## 2、杂谈

### 1、自动求导

```python
import torch
x=torch.arange(10,dtype=torch.float,requires_grad =True)
y=x*x
u=y.detach()
z=u*x
z.sum().backward()
x.grad
```

自动求导时，只能对标量进行该操作，因此需要进行`sum`。 PyTorch 默认在第一次 `.backward()` 后就**释放了计算图**以节省内存。需要注意的是，只有在反向传播之后，才会使用下面的代码，避免梯度累加。如果张量 `a` 没有参与到当前的计算图中，也就没有被自动微分系统记录下来，它的 `.grad` 不会被更新，因此 **不需要清除梯度**。

```
x.grad.zero_()
```

#### 🪼反向传播

反向传播本质上是链式法则（链式求导），从一个**最终输出**反推所有中间变量的梯度。如果输出是**多个值**（比如一个向量），那么：

- 反向传播不止一个方向，不知道你想对哪个输出做梯度；
- 或者你需要提供梯度权重（即 `.backward(gradient=...)`）告诉它怎么组合多个方向。

```
z.sum().backward(retain_graph=True)
```

#### 🪼**叶子张量（Leaf Tensor）**

1. 是 `requires_grad=True`；

2. 不是通过其他 Tensor 运算得来的（也就是说，没有父节点）；

   对于中间张量来说，pytorch并不会保存他的梯度，除非加上下面代码

   ```
   b.retain_grad()  # 👈 必须加这句才能访问 b.grad
   ```


你提出的这个问题，正是神经网络设计中一个非常经典且关键的“对称性陷阱”，理解它对后续学习非常重要。

------

### 2. **对称性**

你有两个“隐藏单元”，它们结构完全一样（同样的输入，权重参数完全相同），这就是所谓的“对称性”。
 换句话说，隐藏单元1和隐藏单元2从初始化开始是一模一样的。

- **前向传播阶段**
  因为参数完全相同，两个隐藏单元输出完全一样的激活值。
- **反向传播阶段**
  梯度也是一样的，更新后的权重依然完全相同。
  这就像你有两个隐藏单元，但它们一直“绑在一起”，变成了**一个单元**的行为。

- 这就导致网络实际表现就像只有一个隐藏单元，失去了多单元带来的表达能力（也就是学不到更复杂的函数）。

------

#### 1.**梯度不会打破对称**

因为更新是基于相同的梯度和相同的参数执行的，导致两个隐藏单元权重更新完全同步。

==解决方法==

- **随机初始化参数**：这是最常用也是最关键的策略。
  让每个隐藏单元从不同的参数开始，保证它们的激活和梯度不同。
- **暂退法（Dropout）等正则化方法**：在训练过程中随机屏蔽部分单元，可以打破这种对称，帮助学习更丰富的表示。

------

#### 2.总结（务实视角）

> **对称性陷阱**：当多个神经元从相同参数和数据开始时，它们的行为和更新是完全一致的，导致模型能力被严重限制。
> **解决方法**：避免所有参数完全相同，通常采用随机初始化；训练中还可以借助 Dropout 等技术打破同步。

“协变量偏移”（Covariate Shift）是机器学习和深度学习中一个非常重要但容易被忽视的问题。理解它能帮助你更好地设计训练流程和模型提升泛化能力。

------

### 3、协变量偏移

协变量偏移指的是：

> **训练数据的输入分布和测试数据的输入分布不一致。**

也就是说，模型在训练时见到的数据特征分布 和在测试或实际应用时遇到的特征分布不一样。

------

#### 为什么会出现协变量偏移？

- 现实中数据采集条件不同（传感器、时间、环境变化等）
- 训练集采样不充分或有偏差
- 数据预处理差异

------

#### 协变量偏移的后果？

- 训练好的模型在测试时性能下降，表现出“泛化差”
- 训练时学到的特征分布和实际应用场景不匹配

------

#### 常见的缓解方法

1. **数据归一化和标准化**
   保证训练和测试数据在同一尺度，减少分布差异。
2. **数据增强（Data Augmentation）**
   通过人为制造多样的训练样本，让模型更健壮。
3. **领域自适应（Domain Adaptation）**
   使用技术让模型适应测试数据的分布，比如对抗训练调整特征分布。
4. **重加权样本（Importance Weighting）**
   训练时根据测试数据分布调整样本权重，缩小分布差异。

------

#### 结合神经网络训练的视角

- 在深度学习中，**内部协变量偏移（Internal Covariate Shift）**更为关注——即训练过程中每一层输入分布不断变化，影响训练稳定性。
- 这也是 **批量归一化（Batch Normalization）** 等技术出现的根本原因，帮助缓解训练过程中的协变量偏移。

------

#### 你可以这样理解

> 协变量偏移就像是在学开车，你在白天（训练数据）练习，到了晚上（测试数据）遇到黑暗和雨天，视线变差，表现自然下滑。

------

“内部协变量偏移”（Internal Covariate Shift，简称ICS）是深度学习训练中非常核心但也颇具争议的概念。理解它有助于你更好地掌握训练稳定性和加速收敛的关键技术，比如批量归一化（Batch Normalization）。下面我帮你务实、清晰地解析这个概念。

### 4、“内部协变量偏移”？

- **定义**：在神经网络训练过程中，随着参数不断更新，**每一层输入的分布也在不断变化**，这就是“内部协变量偏移”。
- 换句话说，层与层之间的数据分布“不断漂移”，给训练带来了额外的不稳定性。

- 当某一层的输入分布频繁变化时，下游层需要不断适应新的输入分布，导致训练过程变得**缓慢且不稳定**。
- 梯度更新容易震荡，收敛速度减慢。

------

#### 这个问题的来源

- 每次参数更新，前面层的输出会改变，从而改变后面层的输入分布。
- 网络越深，这种累积的输入分布变化越严重。

------

#### 批量归一化（BatchNorm）如何解决？

- BatchNorm 通过对每一层的输入在**小批量样本内**做归一化（均值0，方差1），**固定输入的分布**，减少了输入的漂移。
- 这样后续层的输入分布变得稳定，网络训练更快、更稳健。

------

#### 总结

| 关键点         | 说明                             |
| -------------- | -------------------------------- |
| 内部协变量偏移 | 网络训练时每层输入分布不断变化   |
| 影响           | 训练不稳定，收敛慢               |
| 解决办法       | 批量归一化，固定输入分布         |
| 争议           | 机制细节仍有学术讨论，但效果显著 |

------

###  5、概念偏移

我们也可能会遇到*概念偏移*（concept shift）： 当标签的定义发生变化时，就会出现这种问题。 这听起来很奇怪——一只猫就是一只猫，不是吗？ 然而，其他类别会随着不同时间的用法而发生变化。 精神疾病的诊断标准、所谓的时髦、以及工作头衔等等，都是概念偏移的日常映射。

### 6、非平稳分布

- 非平稳分布指的是**数据的分布随时间（或环境）发生变化**，即数据生成的统计特性不是固定的。
- 换句话说，数据不满足独立同分布（i.i.d.）的假设。

------

#### 🦄举个简单的例子

- 一个电商平台的用户购买行为随着季节、促销活动不断变化，导致用户的点击率、购买偏好等数据分布不断变。
- 你训练模型时用的是过去半年数据，但测试时用户行为发生了改变，模型效果自然下降。

------

#### 非平稳分布带来的挑战

- **模型泛化能力受限**：模型基于旧数据学到的规律对新数据失效。
- **训练过程不稳定**：模型参数可能在训练中难以收敛。
- **预测性能波动**：线上模型效果随时间波动大，难以保证稳定。

------

#### 非平稳分布的类型（上面提到了一些）

1. **协变量偏移（Covariate Shift）**
   输入分布变化，条件分布不变。
2. **标签偏移（Label Shift）**
   标签分布变化，条件输入分布不变。
3. **概念漂移（Concept Drift）**
   条件分布本身发生变化，即 P(Y∣X)P(Y|X) 变化。

非平稳分布通常表现为上述一种或多种变化的组合。

------

#### 应对非平稳分布的策略

- **在线学习与增量学习**
  模型不断用最新数据微调，适应分布变化。
- **领域自适应与迁移学习**
  设计模型适应新环境，减少分布差异影响。
- **检测与报警机制**
  监测数据分布的漂移，及时触发模型更新。
- **多模型融合**
  结合多个针对不同时间段训练的模型，提高鲁棒性。

------

#### 你可以这样理解

> 非平稳分布就像河流的水流方向和速度时刻变化，行船者（模型）需要不断调整航向，才能顺利到达彼岸。

------

### 7、可学习参数

```
print(net[2].state_dict())
```

这样，可以打印该层的可学习参数权重

```
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
```

这样，可以得到指定层的偏置，类型和数据，要记住：参数是复合的对象，包含值、梯度和额外信息， 除了值之外，我们还可以访问每个参数的梯度。

```
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
```

可以直接得到全部的参数值

```auto
import torch
from torch import nn

"""延后初始化"""
net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))
# print(net[0].weight)  # 尚未初始化
print(net)

X = torch.rand(2, 20)
net(X)
print(net)
```

延后初始化，也就是定义时，不给出输入维度，让系统根据真实的输入shape去判断。定义层大小时即便唯独不匹配，也不会报错，只有在forward（前向传播）并且需要处理数据时，如果维度不匹配会直接报错（维度匹配是根本）



### 8、保存模型

为什么 **`lambda x: x \* 2`** 这类匿名函数不适合直接放进 `nn.Sequential` 并用 **`torch.save(model)`** 保存整个模型？

| 关键点                                     | 解释                                                         |
| ------------------------------------------ | ------------------------------------------------------------ |
| **PyTorch 保存完整模型依赖 Python pickle** | `torch.save(model)` 实际是把整个模型对象 **pickle** 到磁盘。pickle 记录的并不是“源代码”，而是“在运行时如何找到同名对象”。 |
| **pickle 需要可重定位的“全路径”**          | 当 pickle 遇到一个函数（或方法）时，它存储它的 **module 名 + 对象名**。加载时会做 `import module` 然后用 `getattr(module, name)` 找到同一个对象。 |
| **lambda 没有全路径**                      | 匿名函数的 `__name__` 是 `<lambda>`，而且它通常定义在 `__main__`（交互环境、脚本作用域）里，没有一个可重导入的模块路径；加载时 `import __main__` 后做 `getattr(__main__, '<lambda>')` 必然失败。 |
| **可能还捕获闭包**                         | lambda 往往捕获外部局部变量；pickle 只能保存值，无法重建闭包逻辑，进一步增加不可靠性。 |

演示：为什么 lambda 会出错

```python
import torch
from torch import nn

model = nn.Sequential(
    nn.Linear(4, 4),
    # 放入匿名函数层（会失败）
    lambda x: x * 2                 
)

torch.save(model, "bad.pth")  # 这一步能跑通
model2 = torch.load("bad.pth") # 这里很可能报 pickle 找不到 <lambda>
```

加载时报类似错误：

```
AttributeError: Can't get attribute '<lambda>' on <module '__main__' ...>
```

------

#### 🏞️正确做法：用显式模块来替代 lambda

```python
class TimesTwo(nn.Module):
    def forward(self, x):
        return x * 2

model = nn.Sequential(
    nn.Linear(4, 4),
    TimesTwo()          # 可序列化，因为 TimesTwo 有真正的模块路径
)

torch.save(model, "good.pth")
model2 = torch.load("good.pth")  # 加载成功
```

- `TimesTwo` 定义在某个可导入的 Python 模块（哪怕是当前脚本）。
- pickle 记录 `"your_module_name.TimesTwo"`，加载时能 `import your_module_name` 并拿到同名类，从而恢复对象。

------

#### ⚽面向未来的务实建议

1. **始终把“层”写成 `nn.Module` 子类**
   既避免 lambda 的序列化难题，又能利用 `register_buffer`、`register_parameter` 等高级特性。

2. **生产环境优先保存 `state_dict`**

   ```python
   torch.save(model.state_dict(), "weights.pth")
   ```

   然后 **显式重建结构** 并 `load_state_dict`，跨版本最稳妥。

3. **如果一定要保存完整模型**（原型、快速 demo）：

   - 保证所有自定义层都在可导入的 Python 文件里；
   - 避免 lambda、局部嵌套函数、动态生成类等无法定位的对象。

只要遵循这些约束，你就能既方便地序列化，也能在日后回溯或部署时少踩坑。祝你项目顺利！

### 12、softmax

Softmax 丢掉了特征的绝对信息，仅保留比例信息；若两个特征方向一致或比例相近，Softmax 输出会相似，哪怕它们本质不同。

原输入在经过softmax之后，丢失了原本的特征信息，因为softmax的输出是各个输入在总输入中占的权重比例，那么即使原输入不一样，也很有可能，在总输入中的比例相近。那么，softmax就会得出:这两个输入特征一致，但是事实上，他们并不相同

### 13、==通用架构==

#### 🎋VGG-Net

<img src="C:\Users\osquer\Desktop\typora图片\image-20250627153151729.png" alt="image-20250627153151729" style="zoom: 80%;" />

- VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。
- 块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。
- 在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即3×3）比较浅层且宽的卷积更有效。

#### 🎋Alex-Net

<img src="C:\Users\osquer\Desktop\typora图片\image-20250627153213340.png" alt="image-20250627153213340" style="zoom:80%;" />

1. AlexNet比相对较小的LeNet5要深得多。AlexNet由八层组成：五个卷积层、两个全连接隐藏层和一个全连接输出层。
2. AlexNet使用ReLU而不是sigmoid作为其激活函数。

#### 🎋NIN-Net

<img src="C:\Users\osquer\Desktop\typora图片\image-20250627153125555.png" alt="image-20250627153125555" style="zoom: 80%;" />

- NiN使用由一个卷积层和多个1×1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
- NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。
- 移除全连接层可减少过拟合，同时显著减少NiN的参数。
- NiN的设计影响了许多后续卷积神经网络的设计。

#### 🎋Geogle-Net

![image-20250627153045561](C:\Users\osquer\Desktop\typora图片\image-20250627153045561.png)

- Inception块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用1×1卷积层减少每像素级别上的通道维数从而降低模型复杂度。
- GoogLeNet将多个设计精细的Inception块与其他层（卷积层、全连接层）串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。
- GoogLeNet和它的后继者们一度是ImageNet上最有效的模型之一：它以较低的计算复杂度提供了类似的测试精度。

#### 🎋Dense-Net

- 在跨层连接上，不同于ResNet中将输入与输出相加，稠密连接网络（DenseNet）在通道维上连结输入与输出。
- DenseNet的主要构建模块是稠密块和过渡层。
- 在构建DenseNet时，我们需要通过添加过渡层来控制网络的维数，从而再次减少通道的数量。

#### 🎋LSTM

![image-20250718113432603](C:\Users\osquer\Desktop\typora图片\image-20250718113432603.png)

- 长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。
- 长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。
- 长短期记忆网络可以缓解梯度消失和梯度爆炸。

#### 🎩网络设计心得

1. 网络的输入层，如果size比较大，可以采用较大得kernel size，具有较大的感受野，通常 **7×7 + padding=3 + stride=2**，

2. 1 x 1的卷积核进行逐点卷积，可以保持图像尺寸不变，因为1X1卷积不会看到周围的像素，只会对像素点做线性变换，类似于mlp的效果

3. 使用AGP（AdaptiveAvgPool2d）收尾。这个层对每个通道**独立地计算全局平均**，即对该通道的所有空间位置求平均值，得到一个数值。所以输出是一个 **“每个通道的整体响应强度”**，压缩了空间信息但保留了通道维度的所有信息。换句话说，空间上的细节被“平均掉”了，但每个通道的整体表现依然保留，没有被丢弃。

4. **全局最大池化（Global Max Pooling）**

   - **全局最大池化**是对每个通道取最大值，表达“该通道中是否有特别强烈的激活”。
   - **特点**：
     - 对突出的局部特征敏感，能突出最显著的激活。
     - 适合用于检测“存在性”问题，比如目标检测中判断某类是否出现。
   - **对比平均池化**：
     - 平均池化更稳健，适合捕捉整体的统计信息，降低噪声影响。
     - 最大池化更“激进”，会放大最强响应，忽略其他部分

   | 任务类型             | 推荐池化方式             | 说明                               |
   | -------------------- | ------------------------ | ---------------------------------- |
   | 分类（多类、细粒度） | 全局平均池化（GAP）      | 抓取整体特征，效果稳定             |
   | 检测、定位           | 全局最大池化             | 强调显著区域，有助于定位激活区域   |
   | 结合使用             | 平均池化 + 最大池化 拼接 | 更丰富特征表达，兼顾整体和局部激活 |





### 16、inception块和残差块的对比

#### **Inception 块 vs 残差块：简明对比**

#### **主要区别**

| **特性**     | **Inception 块**                                  | **残差块**                                   |
| ------------ | ------------------------------------------------- | -------------------------------------------- |
| **结构**     | 并行多分支（1x1、3x3、5x5 卷积 + 池化），通道拼接 | 串行卷积（通常 2 个 3x3 卷积）+ 跨层通路相加 |
| **计算方式** | 多尺度特征提取，拼接输出，通道数增加              | 残差学习（`y = F(x) + x`），通道数可变       |
| **设计目标** | 宽度优先：捕获多尺度特征，优化计算效率            | 深度优先：解决深层网络退化，支持更深网络     |
| **输出形状** | 通道数增加，空间尺寸通常不变                      | 通道数可变，空间尺寸可能减小（步幅 > 1）     |

#### **Inception 块特征**

- **并行分支**：1x1、3x3、5x5 卷积和池化，捕获多尺度特征。
- **1x1 卷积降维**：减少计算量，优化效率。
- **通道拼接**：输出通道数为各分支之和，丰富特征表达。
- **应用**：复杂视觉任务（如目标检测、细粒度分类），如 GoogLeNet。

#### **残差块特征**

- **残差连接**：主路径输出 `F(x)` 与输入 `x` 相加，学习残差。
- **跨层通路**：Identity 或 1x1 卷积，确保形状匹配。
- **支持深层网络**：缓解梯度消失和退化，适合极深网络。
- **应用**：图像分类、检测等深层任务，如 ResNet-18/50。

#### **总结**

- **Inception**：强调宽度，多尺度特征提取，计算复杂但高效。
- **残差块**：强调深度，残差学习支持深层网络，结构简单稳定。
- **结合**：Inception-ResNet 融合两者，兼顾宽度和深度。

### 17、序列模型

- 内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此，对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。
- 序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。
- 对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。
- 对于直到时间步的观测序列，其在时间步的预测输出是“步预测”。随着我们对预测时间值的增加，会造成误差的快速累积和预测质量的极速下降。

### 18、模型语言和数据集

最流行的词看起来很无聊， 这些词通常被称为*停用词*（stop words），因此可以被过滤掉。 尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。 此外，还有个明显的问题是词频衰减的速度相当地快。 例如，最常用单词的词频对比，第个还不到第个的。 

语言模型是自然语言处理的关键。

- 元语法通过截断相关性，为处理长序列提供了一种实用的模型。
- 长序列存在一个问题：它们很少出现或者从不出现。
- 齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他元语法。
- 通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。
- 读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。

![image-20250705150508506](C:\Users\osquer\Desktop\typora图片\image-20250705150508506.png)

### 19、困惑度

一个更好的语言模型应该能让我们更准确地预测下一个词元。 因此，它应该允许我们在压缩序列时花费更少的比特。 所以我们可以通过一个序列中所有的个词元的交叉熵损失的平均值来衡量：
$$
\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),
$$
其中由P语言模型给出， xt是在时间步从该序列中观察到的实际词元。 这使得不同长度的文档的性能具有了可比性。 由于历史原因，自然语言处理的科学家更喜欢使用一个叫做*困惑度*（perplexity）的量。
$$
\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).
$$

------

#### 🎯 困惑度是什么？一句话定义：

> **困惑度 = 模型对下一个词“不确定”的程度。数值越小越好。**

------

#### 🔍 更准确地说：

困惑度（Perplexity）是语言模型对一个句子的“平均不确定性”的度量，具体公式如下：
$$
Perplexity=exp⁡(−1T∑t=1Tlog⁡P(wt∣w1,...,wt−1))= \exp\left( -\frac{1}{T} \sum_{t=1}^T \log P(w_t \mid w_1, ..., w_{t-1}) \right)
$$
其中：

- TT 是序列长度；
- P(wt∣w1,...,wt−1)P(w_t \mid w_1,...,w_{t-1}) 是模型预测当前词 wtw_t 的概率。

> 换句话说，它就是交叉熵损失的**指数形式**。

------

#### 🔢 用例子理解困惑度

假设词表中有 10000 个词元。

#### ✅ 最理想的模型：

- 每次都把**正确的下一个词**预测概率为 1，其他为 0；
- 那么 log(1) = 0，困惑度 = exp(0) = **1**。

> ✅ 完全没有“困惑”——它完全确定答案。

------

#### ❌ 最差的模型：

- 每次都把正确词元的概率预测为 0；
- log(0) 是负无穷，对数损失无穷大 → 困惑度 = **正无穷**。

> ❌ 永远预测错，彻底“迷惑”。

------

#### ⚠️ 基线模型（完全随机）：

- 假设词表大小为 V = 10000；
- 每个词的概率是 1/10000；
- 那么困惑度 = 10000。

> 📌 这个时候模型“平均每次”都在从 10000 个词中等概率地瞎选 —— 它的困惑度就等于词表大小。

------

#### 📦 所以你说的这句话是？

> 困惑度的最佳理解是：“下一个词元的**实际选择数**的调和平均数”。

这句话可以这样理解：

- 假设你在预测句子：“I want to eat ___.”

  - 模型分配了一个概率分布：比如：

    ```
    pizza: 0.6
    rice:  0.3
    book:  0.1
    ```

  - 那么实际上它只“有效使用”了这几个词元（也就是只关注了3个词，其他忽略不计）；

  - 困惑度会反映这个分布的集中程度：如果概率越集中，困惑度就越小；

  - 如果模型把概率分得很平均（比如每个词都 0.01），就说明它“什么都不确定”，困惑度高。

> 所以：**困惑度越低，代表模型越确定“我知道下一个词是哪个”**。

------

#### 📊 总结一下：

| 情况     | 描述             | 困惑度                     |
| -------- | ---------------- | -------------------------- |
| 理想情况 | 模型预测100%正确 | 1                          |
| 完全随机 | 词表均匀分布     | 词表大小 V                 |
| 完全错误 | 总是错           | 正无穷                     |
| 实际模型 | 介于两者之间     | 一般在 20~500+，取决于任务 |

------

#### 💡 实用建议

- 训练语言模型时，如果你在监控困惑度，看到它下降就说明模型在“学会预测下一个词”。
- 若困惑度长期停留在词表大小附近，说明模型还没学到有用的信息。
- 可以把困惑度当作一个“可解释的准确率指标”。

### 20、隐状态

**隐藏层**是网络的空间维度——处理“这一次输入”要经过哪些算子。 **隐状态**是网络的时间维度——上一次处理结果如何影响“下一次输入”。RNN 的参数矩阵（例如用于计算隐藏状态的权重）在每次训练结束后才会更新。而“承载先前状态”的是隐藏状态本身（h_t），不是这个矩阵。

- **隐状态（hidden state）** 是模型在每个时间步计算出的中间变量，它承载了对序列历史信息的记忆，**用于前向传播时传递信息**。
- 它本身**不直接作为参数**，**不存储梯度**，所以不会被直接更新，也不会参与梯度下降。
- 但是，隐状态是通过权重矩阵（比如你代码里的 `w_hh` 和 `w_xh`）和输入计算出来的，权重矩阵是**模型的可训练参数**，在反向传播时会计算梯度并更新。
- 换句话说，隐状态是“动态生成”的中间结果，权重矩阵才是“静态”的、训练时需要优化的东西。
- 你可以理解为隐状态通过权重矩阵间接反映了模型的“记忆”，权重矩阵的调整影响隐状态的变化和模型的表现。

这也是RNN结构的核心：状态的传递保证了时间维度上的信息积累，而权重矩阵的优化保证了模型性能的提升。

### 21、 `prefix` 预热

定义预测函数来生成`prefix`之后的新字符， 其中的`prefix`是一个用户提供的包含多个字符的字符串。 在循环遍历`prefix`中的开始字符时， 我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。 这被称为*预热*（warm-up）期， 因为在此期间模型会自我更新（例如，更新隐状态）， 但不会进行预测。 预热期结束后，隐状态的值通常比刚开始的初始值更适合预测， 从而预测字符并输出它们。

RNN 是有**记忆能力**的，隐状态 `H` 是对历史输入的“总结”。如果你一开始就随机初始化状态（`H=0`），预测是盲目的。用 prefix 走一遍，把 `H` 更新成“上下文相关”的，预测才有语义意义。

### 22、隐状态和模型参数

别把 **“隐状态 H”** 和 **“模型参数 W”** 混在一起——它们是两条完全不同的链：

| 概念                        | 作用                       | 生命周期                        | 是否 `detach()`                  | 是否跨 epoch 传递                                            |
| --------------------------- | -------------------------- | ------------------------------- | -------------------------------- | ------------------------------------------------------------ |
| **模型参数**`W_xh, W_hh, …` | 网络的可训练权重           | **整个训练过程** 都存在         | **不**；反向传播要用它们的梯度   | **会**：优化器更新后直接写回，同一个 `net` 在下一 epoch 继续用更新后的参数 |
| **隐状态**`H_t`             | RNN 当下记忆，用来算下一步 | **在一个批次的前向过程中** 持续 | **要**（顺序分区时）让梯度别跨批 | **通常不会**：epoch 之间重新置零                             |

------

#### 训练一轮（epoch）时发生什么？

1. **前向计算**

   - 用当前权重 WW 与隐状态 HH 计算输出。

2. **反向传播**

   - 计算损失对各层权重的梯度 ∇W\nabla W（此时 `H.detach()` 只影响梯度**链条长度**，不影响 ∇W\nabla W 的数值正确性）。

3. **优化器步进**

   ```python
   optimizer.step()    # W ← W - lr * ∇W
   optimizer.zero_grad()
   ```

   - **权重 W 被真正修改并保存在 `net` 内部**。

4. **下一个批次 / epoch**

   - 用更新后的 WW 继续前向，模型能力逐步提升。
   - **隐状态 H** 在跨批(顺序分区)可延续其数值、跨 epoch 一般重置为 0；它不是参数，不参与 optimizer 更新。

------

#### 一目了然的小循环伪码

```python
for epoch in range(num_epochs):
    state = net.begin_state(batch_size, device)  # 隐状态通常每轮重置
    for X, Y in train_iter:                      # 遍历所有 batch
        # ---- forward ----
        y_hat, state = net(X, state)             # state.detach() 已在 net 内做
        loss = criterion(y_hat, Y)

        # ---- backward & update ----
        loss.backward()                          # 计算 ∇W
        torch.nn.utils.clip_grad_norm_(net.params, theta)  # (可选)梯度裁剪
        optimizer.step()                         # ← 真正写回 W
        optimizer.zero_grad()
```

- `optimizer.step()` **写回修改后的参数 W**，所以 **权重自动随 epoch 累积效果**。
- `state` 只是这一批的记忆，“借壳”来帮助前向计算；`detach()` 只影响它的梯度链，不影响 `W`。

------

#### 核心记忆点

> **`detach()` 只是让 \*梯度\* 不再往更早的时间步传，
> 它不会阻止 \*权重\* 在 optimizer.step() 时被永久更新，
> 下一 epoch 加载的仍然是“上轮更新后”的权重。**

这样就能既控制梯度爆炸，又让参数持续学习。

### 23、把 **“梯度链”** 和 **“权重张量”** 区分开来

| 概念               | 作用                            | 生命周期                      | 与 `detach()` 的关系                        |
| ------------------ | ------------------------------- | ----------------------------- | ------------------------------------------- |
| **梯度计算图**     | 记录前向算子，用来反向传播误差  | 仅存在于**一次前向‑反向周期** | `detach()` **剪断**它，使反向传播停在剪断点 |
| **权重张量** (`W`) | 网络可训练参数，存储在 `net` 里 | 整个训练过程中持续存在        | `detach()` **不会**影响它的存储或更新       |

------

#### 1 为什么要 `detach()`？

```python
state = state.detach()
```

- 目的是**断开梯度反向传播链条**，让接下来的 `loss.backward()` 只在 **当前小批量的时间步** 内展开。
- **只是对梯度的“图结构”动刀**；`state`（数值）还在，下一批采用这同一份 `H` 值继续前向。

------

#### 2 权重何时更新？

```python
loss.backward()   # 计算 dL/dW
optimizer.step()  # W ← W - lr * dL/dW
```

- `optimizer.step()` **就地修改** `W` 的 `.data`，新的数值立即写回。
- 这一步**与梯度链条是否被 detach 毫无冲突**——它只看 `W.grad` 里的数值。

------

#### 3 跨 epoch 的连贯性

- **epoch 结束**：我们通常 `state = None` 或重新置 0，但 **`net` 仍然保留更新后的权重**。
- **下一 epoch 开始**：重用 **同一个 `net` 对象**，它的权重已经是“上一轮学习后”的新值，于是继续收敛。

```text
Epoch 1
  forward → backward → optimizer.step()  (W₁ → W₂)
Epoch 2
  forward uses W₂
```

------

#### 4 一句核心对照

| 操作               | 影响                                            |
| ------------------ | ----------------------------------------------- |
| `detach()`         | **剪断梯度传播路径**，限制反向传播深度          |
| `optimizer.step()` | **更新权重数值**，持久写入 `net`，跨 epoch 保留 |

两者操作对象完全不同，因此“`detach()` 不会阻止权重更新”——权重依旧在每个 batch 后被写回；而“权重更新”也不会破坏你对梯度链深度的控制。

------

#### ⌛ 口诀

> **梯度链用 `detach` 割，
> 权重值由 `step` 改，
> 割链不割权，权改值长存。**
>
> 

### 24、向前传播与反向传播

==关键点：**计算图是“边建边连”的**，而不是“只在 backward 那一刻才存在”==

1. **每一次前向运算**（矩阵乘、加法、`tanh` …）
   PyTorch 都会即时在内存里创建一个小节点，串进**当前这一条链**。
2. **如果你连续调用多次前向**，并且这些运算之间 **有张量依赖**，
   那么这几条小链就会**拼成一条更长的链**。
   - 在顺序分区的 RNN 里：
     - **Batch 1** 把隐状态 `H₁_end` 传给 **Batch 2** 作输入；
     - Batch 2 的图就会把 **Batch 1 的整条链也挂进来**，
       于是整段梯度路径 = Batch₁+Batch₂ 的总长度。
3. **直到你调用 `loss.backward()`**，PyTorch 才沿着这“一整条链”回溯并释放图。
   - 如果这条链横跨了几个 batch，就变得又长又占显存，还可能梯度爆炸。

------

#### 所以要 `detach()` 做什么？

```python
state = state.detach()
```

- **把 `state` 的“计算历史”切断**：
  - `state.data` 的数值保留给下一个 batch 当输入；
  - 但它不再指向之前那一大串算子节点。
- 下一个 batch 的前向会**从这根新的“断点”重新开始建图**，
  于是 **反向传播只需要翻过当前 batch 的 `num_steps` 步**，
  既省显存，也避免跨批次爆炸。

> 可以把它想成：**“把已有纸带剪断，新的记录从空白处开始写”**。纸带内容（数值）还在，但旧纸带上的墨迹（梯度可追溯链）被封存，不再连下去。

------

#### 如果不 `detach()` 会怎样？

- Batch 1 图 + Batch 2 图 + …
  一直拖到你调用 `backward()` 的时刻才整体回溯。
- 对 RNN，可能是 **几十上百步**，显存直线上升；
  还可能出现梯度爆炸/消失。

------

#### 核心结论

| 误区                                     | 正解                                                         |
| ---------------------------------------- | ------------------------------------------------------------ |
| “计算图只在 backward 时才有，所以不用管” | ❌ 前向时就**即时构建**，会跨 batch 延长                      |
| “`detach()` 会丢失数值”                  | ❌ 只切梯度链，**数值不变**                                   |
| “剪断后就学不到长依赖”                   | ✅ 只能学到 `num_steps` 步内的依赖；这是**截断 BPTT 的设计权衡** |

通过 `detach()`，我们把**一次反向传播的跨度**控制在可以承受的范围，而权重依然在每个 batch 后被 `optimizer.step()` 持续更新，这就是截断 BPTT 的意义所在。

### 25、BPTT

------

#### 🔁 什么是 BPTT（Backpropagation Through Time）？

BPTT = **时间上的反向传播**

是 RNN（循环神经网络）中训练模型时的核心算法，它是普通的反向传播（Backpropagation，BP）在时间维度上的“扩展”。

------

#### 🧠 直觉类比

普通神经网络是“深层前馈”：

```
x → L1 → L2 → L3 → y
     ↑    ↑    ↑
   grad grad grad
```

RNN 则是“时间展开”，你可以理解为一个网络复制了多份，展开在时间轴上：

```
x₁ → h₁ → y₁  
     ↓  
x₂ → h₂ → y₂  
     ↓  
x₃ → h₃ → y₃  
... 时间轴 ...
```

每个时间步之间的连接是通过**隐藏状态** `hₜ` 实现的。

------

#### ✅ BPTT 步骤

当我们训练 RNN 时，损失是整个序列的：

```python
loss = loss₁ + loss₂ + loss₃ + ...
```

#### BPTT 做的是：

1. 正向传播：逐步计算每个时间步的输出、隐藏状态
2. 反向传播：从最后一个时间步 **向前反传每个时间步的梯度**
   - `∂L/∂W`, `∂L/∂hₜ`, `∂L/∂xₜ`，等等
3. 参数更新：基于累积的梯度更新模型

------

#### 💣 为什么 BPTT 有挑战？

- 计算图太长：比如一个长度 100 的序列，反向传播要展开 100 步
- 容易导致 **梯度爆炸** 或 **梯度消失**
- 显存压力极大

------

#### ✂ 截断 BPTT（Truncated BPTT）

为了解决上述问题，实际训练中我们会限制时间步长度，比如：

```python
num_steps = 35
```

每次只在这 35 步内计算反向传播，**其余的历史用 `detach()` 剪断**，这就叫：

> Truncated BPTT（截断时间上的反向传播）

------

#### ✍️ 总结一句话：

> **BPTT 就是 RNN 训练时在时间维度上的反向传播**，
> 为了避免计算/内存/梯度爆炸问题，常常使用“截断 BPTT”，每 `num_steps` 步就 `detach()` 一次，控制梯度链长度。



### 26、模型训练流程

| 模块       | 关键词                              | 解释                                 |
| ---------- | ----------------------------------- | ------------------------------------ |
| 前向传播   | `model(input)` → `output`           | 将输入数据传入神经网络，得到预测输出 |
| 损失函数   | `loss(output, label)`               | 用于评估预测结果与真实标签之间的误差 |
| 反向传播   | `loss.backward()`                   | 自动计算损失对每个参数的“梯度”       |
| 梯度清零   | `optimizer.zero_grad()`             | 每次更新前都要清除旧梯度，避免累加   |
| 梯度剪裁   | `torch.nn.utils.clip_grad_norm_()`  | 限制梯度过大，防止梯度爆炸           |
| 参数更新   | `optimizer.step()`                  | 按梯度方向更新模型的参数             |
| 隐状态管理 | RNN中如`state.detach()`或重新初始化 | 控制隐藏状态的传递与更新方式         |



### 27、RNN中的常用属性

| 属性名           | 说明                                             |
| ---------------- | ------------------------------------------------ |
| `input_size`     | 输入特征的维度（每个时间步的输入大小）           |
| `hidden_size`    | 隐藏状态的特征维度（隐藏层神经元数量）           |
| `num_layers`     | 堆叠的RNN层数                                    |
| `nonlinearity`   | 非线性激活函数类型，`'tanh'` 或 `'relu'`         |
| `bias`           | 是否使用偏置，布尔值                             |
| `batch_first`    | 输入输出张量的第一个维度是否为batch size，布尔值 |
| `dropout`        | 除最后一层外，其他层的dropout概率                |
| `bidirectional`  | 是否是双向RNN，布尔值                            |
| `weight_ih_l[k]` | 第k层输入到隐藏层的权重（参数张量）              |
| `weight_hh_l[k]` | 第k层隐藏到隐藏层的权重（参数张量）              |
| `bias_ih_l[k]`   | 第k层输入到隐藏层的偏置                          |
| `bias_hh_l[k]`   | 第k层隐藏到隐藏层的偏置                          |

### 28、`tanh` 和 `sigmoid`对比

#### ✅ tanh：用于候选记忆元

$$
\tilde{C}_t
$$



- **值域：**[−1,1]
- **作用：** 能表示正向和负向的激活，适合用来“编码复杂的信息”。
- **原因：** 候选记忆元是**新信息的载体**，需要有能力表达“增强”与“抑制”（正负信号），因此选择了 `tanh`。

👉 **总结一句话：**
 `候选记忆元` 不是门，而是“内容”，需要表达正负信息，选 `tanh` 比较合理。

------

#### ✅ sigmoid：用于门控（输入门、遗忘门、输出门）

- **值域：** [0,1]
- **作用：** 表示“通过程度”（0代表不通过，1代表完全通过）
- **原因：** 门控结构本质上是“滤波器”或者“掩码”，在做“选择性通行”，因此需要的是“比例控制”。

👉 **总结一句话：**
 门控层负责控制信息“通过多少”，使用 sigmoid 实现“0~1之间的比例控制”是最自然的选择。

### 29、`LSTM ` 和  `GRU`

在普通的 RNN 中，隐藏状态的更新是直接的，这样会造成梯度爆炸或消失，难以学习长期依赖。为了解决这个问题，LSTM 和 GRU 引入了**门控机制**，让网络“有选择地”保留旧信息或接纳新信息。

因此，引入了一个“**候选值**”，就是“我准备更新的内容”，但是否真正写入状态（隐状态或记忆元），要看门控的控制。

------

#### 🧠 二、LSTM 中的“候选记忆元参数”详解

LSTM 中有两个状态：

- hth_t：隐状态
- ctc_t：**记忆单元（记忆元）**

更新公式如下：
$$
\begin{align*} f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \quad\text{(遗忘门)} \\ i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \quad\text{(输入门)} \\ \tilde{c}_t &= \tanh(W_c \cdot [h_{t-1}, x_t] + b_c) \quad\text{(**候选记忆元**)} \\ c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \quad\text{(更新记忆元)} \\ o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \quad\text{(输出门)} \\ h_t &= o_t \odot \tanh(c_t) \quad\text{(更新隐状态)} \end{align*}
$$
🔑 **解释**：

- $$
  \tilde{c}_t是候选记忆元：是当前时刻想要写入的内容。
  $$

- 但是否真的写入，要乘上 it（输入门），这是个介于0到1之间的权重。

- 所以，“候选记忆元”提供**新内容的来源**，但真正“记不记”要看门。

#### ✅ 作用：

- **防止信息盲目进入记忆单元**；
- 提高对信息更新的控制能力；
- 保证长期依赖信息不会轻易丢失。

------

#### 🌀 三、GRU 中的“候选隐状态参数”详解

GRU 没有显式的记忆元，只更新隐状态 hth_t，它的结构更紧凑：
$$
\begin{align*} z_t &= \sigma(W_z \cdot [h_{t-1}, x_t] + b_z) \quad\text{(更新门)} \\ r_t &= \sigma(W_r \cdot [h_{t-1}, x_t] + b_r) \quad\text{(重置门)} \\ \tilde{h}_t &= \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h) \quad\text{(**候选隐状态**)} \\ h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \quad\text{(更新隐状态)} \end{align*}
$$
🔑 **解释**：

- $$
  \tilde{h}_t
  $$

  是候选隐状态：当前输入产生的新信息；

- zt控制它与旧隐状态h t-1的比例。

#### ✅ 作用：

- $$
  \tilde{h}_t
  $$

   是模型“希望”作为当前状态的内容；

- 但最终是否使用它由 ztz_t（更新门）决定。

------

#### ✅ 四、总结：为什么要有“候选状态”？

| 类型 | 候选参数         | 作用               | 是否最终使用由谁决定 |
| ---- | ---------------- | ------------------ | -------------------- |
| LSTM | ct（候选记忆元） | 提供新记忆内容     | 输入门 iti_t         |
| GRU  | ht（候选隐状态） | 提供新隐藏状态内容 | 更新门 ztz_t         |

------

### 30、==端到端== 和 ==序列到序列==

------

#### ✅ 端到端学习（End-to-End）

- **定义**：输入直接映射到输出，中间无手工特征。
- **特点**：训练一个整体模型完成全部任务。
- **示例**：语音 → 文本、图像 → 标签。

------

#### ✅ 序列到序列学习（Seq2Seq）

- **定义**：输入输出都是**序列**（长度可变）。
- **结构**：编码器 + 解码器。
- **示例**：机器翻译、文本摘要。

### 31、初识注意力模型

------

✅ 是的，现代的主流网络（尤其是注意力机制下的模型）**不再只依赖编码器“最后一个时刻”的隐状态**来传递信息，而是：

> **传递编码器的每一个时刻的隐状态**（也叫“中间状态序列”或“序列特征”），并让解码器通过“注意力”机制，从中**动态选取重要部分**，生成每个时刻所需的“上下文”向量。

------

#### ❓如果只传最后一个时刻的状态会怎样？

📌 这种做法在最早的 Seq2Seq 模型（如 2014 年 Sutskever 提出的）中是这样做的：

```python
# 编码器输出为一个最后的状态
encoder_final_state = encoder(X)[1]

# 解码器以该状态作为初始状态开始生成序列
decoder_state_0 = encoder_final_state
```

🔍 这种方式的 **缺点** 很明显：

1. **信息压缩瓶颈（信息 bottleneck）**
   - 编码器必须把整个输入序列的信息压缩进一个固定大小的向量中（最后一个隐状态）。
   - 对于长句子或复杂结构的信息，这种压缩是不够的，**会导致丢失上下文细节**。
   - 比如在翻译长句时，解码器根本“记不住”句首的信息。
2. **对长序列性能下降**
   - 随着输入句子变长，最后一个状态包含的信息变得越来越“稀释”，性能显著下降。
3. **不能动态选择关注点**
   - 解码器只能依赖那一个状态，**无法灵活决定“我现在要关注输入序列的哪一部分”**。

------

#### ✅ 为什么注意力机制更好？

✅ 具体做法：

- 编码器输出整个序列的隐状态（shape: `[seq_len, batch_size, hidden_dim]`）
- 解码器每个时间步都通过注意力，从这个序列中“挑选”最相关的信息，构造上下文向量
- 这个上下文再和上一个时间步的解码状态、预测结果组合，输入下一步

✅ 好处：

| 特性       | 传统 Seq2Seq | 注意力机制               |
| ---------- | ------------ | ------------------------ |
| 信息来源   | 最后一个状态 | 所有编码状态             |
| 动态性     | 无           | 每个时间步动态决定注意点 |
| 适应长序列 | 差           | 强                       |
| 翻译质量   | 下降明显     | 提升明显                 |
| 可解释性   | 差           | 高（能看到关注哪些词）   |

------

#### 📌 小结对比图（你可以记住这个关键演变）

```
早期模型：
[ h1 → h2 → h3 → h4 ] → h4 → 解码器

注意力模型：
[ h1, h2, h3, h4 ] → Attention → 每一步动态上下文 → 解码器
```

------

🚀 最后总结一句：

> **“只传最后一个状态”是一种压缩式思维，而“传所有状态 + 注意力”是一种信息解耦 + 动态聚合的更现代策略。**

你也可以把注意力理解为“让解码器拥有搜索输入上下文的能力”，不再死记硬背压缩结果。

------

### 32、🦄比较卷积神经网络、循环神经网络和自注意力这几个架构的计算复杂性、顺序操作和最大路径长度。

#### 1. **计算复杂性**

- **CNN**：
  $$
  O(l \cdot n \cdot k \cdot d \cdot c)
  $$


  - 卷积操作，线性依赖序列长度 \( n \)，卷积核大小 \( k \)，层数 \( l \)，隐藏维度 \( d \)，通道数 \( c \)。

- **RNN**：
  $$
  O(n \cdot d^2)
  $$

  - 逐时间步计算，复杂度随 \( n \) 和隐藏维度 \( d \) 增加，注意力机制加剧 \( n^2 \) 项。

- **自注意力**：
  $$
  O(n^2 \cdot d + n \cdot d^2)
  $$

  - 点积 \( QK^T \) 和加权 
    $$
    \text{softmax}(QK^T)V
    $$
    主导，复杂度随 \( n^2 \) 增长。



#### 2. **顺序操作**

- **CNN**：无，卷积并行处理所有位置，适合 GPU 加速。
- **RNN**：有，时间步顺序计算，限制并行性。
- **自注意力**：无，矩阵操作并行计算所有位置，高效。

#### 3. **最大路径长度**

- **CNN**：\( O(l) \)，感受野随层数 \( l \) 线性增加。
- **RNN**：\( O(n) \)，依赖序列长度 \( n \)，长序列易丢失信息。
- **自注意力**：\( O(1) \)，直接建模任意位置依赖，适合长序列。

#### 4. **实现要点**

- **CNN**：
  - 卷积核滑动提取局部特征，池化降维，全连接输出。
  - 例：`nn.Conv1d(num_hiddens, num_hiddens, kernel_size=3)`。
- **RNN**（如 LSTM）：
  - 逐时间步更新隐藏状态（`h_n, c_n`），结合注意力（如 `AdditiveAttention`）。
  - 例：`nn.LSTM(embed_size + num_hiddens, num_hiddens, num_layers)`。
- **自注意力**（如 MultiHeadAttention）：
  - 点积计算注意力权重，多头并行捕捉特征，线性变换输出。
  - 例：`d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens, num_hiddens, num_heads, dropout)`。

#### 5. **上下文总结**

- **CNN**：高效并行，适合局部特征提取，长距离依赖需深层。
- **RNN**：顺序处理，适合短序列，注意力机制提升性能但仍受限于 \( O(n) \) 路径。
- **自注意力**：并行高效，路径长度 \( O(1) \)，适合长序列，但计算成本高（\( O(n^2 \cdot d) \)）。

---

### 33、模型的基本属性

- `children()` = 直接子模块（外层结构）。
- `modules()` = 所有模块（递归）。



## 3、优化算法详解

### 1、综述

优化和深度学习的目标是根本不同的，优化主要关注的是最小化目标，而后者则关注在给定有限有数据量的情况下寻找合适的模型。训练误差与泛化误差通常不同，由于优化算法的目标通常是基于训练数据集的损失函数，因此优化的目标是减少训练误差。但是，深度学习的目标是减少泛化误差，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合。

深度学习中，大多数目标函数都很复杂，没有解析解只有数值解，因此我们必须使用数值优化算法。然而却存在着不小的挑战，比如局部最小值、鞍点、梯度消失。

---

==<  局部最小值  >== 对于任何目标函数，如果在某一点处值都小于其他点处的值，那么该点就是局部最小值点，对应的值就是局部最小值。倘若该最小值是整个域中目标函数的最小值，那么这一值就被称为全局最小值。

当优化问题的数值接近局部最优值时，随着目标函数解的梯度接近或变为零，最终迭代获得的数值解仅使目标函数局部最优，而不是全局最优。只有一定程度的噪声可能会使参数跳出局部最小值，同样，理论上也有可能跳出全局最小值。然而，现实中，深度学习几乎没有**唯一且准确**的全局最小值，最小值往往不是点，而是一片**低损平原**，容错性很好，换言之，只要模型处于这一片区域的某个点，性能就已经足够优秀了。

---

==<  鞍点  >==  函数的所有梯度消失但既不是全局最小值也不是局部最小值的任何位置，如下所示。此时，该点周围的点既有大数值点，又有小数值点。

![../_images/output_optimization-intro_70d214_66_0.svg](https://zh.d2l.ai/_images/output_optimization-intro_70d214_66_0.svg)

对于高维问题，至少部分特征值为负的可能性相当高，这就使得鞍点比局部最小值更有可能出现。高维特征图如下所示。

![../_images/output_optimization-intro_70d214_81_0.svg](https://zh.d2l.ai/_images/output_optimization-intro_70d214_81_0.svg)

---

==<  梯度消失  >==  多个很小的值连乘时出现的梯度很小，近似于无的现象，导致模型无法更新。

---

### 2、梯度下降

==<  本质  >==  

1. **单维梯度下降**

![image-20250725170403684](C:\Users\osquer\Desktop\typora图片\image-20250725170403684.png)

2. **多维梯度下降**

![   ](C:\Users\osquer\Desktop\typora图片\image-20250725170636818.png)

==<  学习率 （learning rate） >== 决定目标函数能否收敛到局部最小值，以及何时收敛到最小值。选择“恰到好处”的学习率是很棘手的。 如果我们把它选得太小，就没有什么进展；如果太大，得到的解就会振荡，甚至可能发散。所以学习率的选取出现了两个方向：预先确定学习率 或 完全不必选择学习率。同时，预处理有助于调节比例。

### 3、随机梯度下降

相比于梯度下降，随机梯度每次下降引入了噪声，每次处理一个样本。也就是说，即使我们接近最小值，我们仍然会受到随机噪声注入的梯度的不确定性的影响，并且这一现象并不会随着时间得到改善，所以我们需要改变学习率。但是，如果我们一开始设置的学习率过小，就不会得到有意义的进展，如果学习率过大，模型将陷入震荡的状态。所以一个好的解决方案是：模型的学习率能够动态减小。下方是设置学习率的基本策略。
$$
\begin{split}\begin{aligned}
    \eta(t) & = \eta_i \text{ if } t_i \leq t \leq t_{i+1}  && \text{分段常数} \\
    \eta(t) & = \eta_0 \cdot e^{-\lambda t} && \text{指数衰减} \\
    \eta(t) & = \eta_0 \cdot (\beta t + 1)^{-\alpha} && \text{多项式衰减}
\end{aligned}\end{split}
$$

### 4、小批量随机梯度下降

相比于前面的梯度更新方法，小批量随机梯度下降可以充分利用GPU或者CPU的多线程能力，同时对一个 batch 里面的数据同时计算，又同时兼具随机梯度下降的优点。
$$
\mathbf{g}_{t, t-1} = \partial_{\mathbf{w}} \frac{1}{|\mathcal{B}_t|} \sum_{i \in \mathcal{B}_t} f(\mathbf{x}_{i}, \mathbf{w}_{t-1}) = \frac{1}{|\mathcal{B}_t|} \sum_{i \in \mathcal{B}_t} \mathbf{h}_{i, t-1}.
$$

| 符号      | 含义                                                       |
| --------- | ---------------------------------------------------------- |
| g ~t,t−1~ | 第 tt 次迭代中，对上一轮权重 wt−1的梯度估计                |
| B~t~      | 第 t 个 mini-batch 的样本集合                              |
| f~(xi,w)~ | 损失函数，例如 MSE、交叉熵等                               |
| w~t−1~    | 第 t−1次迭代时的模型参数（如权重）                         |
| $∂/∂w$    | 对参数 w 求导                                              |
| h~i,t−1~  | 第 t−1次迭代中，第 i个样本对应的梯度项：∇~w~f(x~i~,w~t−1~) |



### 5、动量法

==<  泄露平均值  >==  是一种**带有衰减权重的滑动平均**，它不像普通平均那样平均所有历史值，而是对新值赋予更多权重、对旧值逐渐“遗忘”。

我们用泄露平均值取代梯度计算,其中$ \beta ∈(0,1)$​ 。

$ \begin{split}\begin{aligned}
\mathbf{v}_t &\leftarrow \beta \mathbf{v}_{t-1} + \mathbf{g}_{t, t-1}, \\
\mathbf{x}_t &\leftarrow \mathbf{x}_{t-1} - \eta_t \mathbf{v}_t.
\end{aligned}\end{split}$

这将有效地把瞬时梯度替换为多个“过去”梯度的平均值，v 被称为动量，它累积了过去的维度，我们可以递归地将v~t~进行扩展。$ \begin{aligned}
\mathbf{v}_t = \beta^2 \mathbf{v}_{t-2} + \beta \mathbf{g}_{t-1, t-2} + \mathbf{g}_{t, t-1}
= \ldots, = \sum_{\tau = 0}^{t-1} \beta^{\tau} \mathbf{g}_{t-\tau, t-\tau-1}.
\end{aligned}$

其中，较大的 $\beta$ 相当于长期平均值，而较小的 $ \beta $只是相对于梯度法略有修正。新的梯度替换不再指向特定实例下降最陡的方向，而是指向过去梯度加权平均值的方向。由于对过去的数据进行了指数降权，有效梯度数为 $ \frac{1}{1-\beta}$​

### 6、$AdaGrad$  算法

==<  稀疏特征  >==  某些特征出现的频率比较小，并且只有当这些不常见的特征出现时，与其相关的参数才会得到有意义的更新，最终出现常见特征相当迅速地收敛到最佳值，而对于不常见的特征，我们仍缺乏足够地观测以确定其最佳值。换言之，学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。

解决此问题的一个方法是记录我们看到特定特征的次数，然后将其用作调整学习率。 即我们可以使用大小为 $ \eta_i = \frac{\eta_0}{\sqrt{s(i, t) + c}}$ 的学习率。对于本节的`AdaGrad` 算法来说，它使用了 $ s(i, t+1) = s(i, t) + \left(\partial_i f(\mathbf{x})\right)^2$  来调整学习率。 它会随梯度的大小自动变化。通常对应于较大梯度的坐标会显著缩小，而其他梯度较小的坐标则会得到更平滑的处理。我们使用变量s~t~ 去累加过去的梯度方差。具体公式如下：

$\begin{split}\begin{aligned}
    \mathbf{g}_t & = \partial_{\mathbf{w}} l(y_t, f(\mathbf{x}_t, \mathbf{w})), \\
    \mathbf{s}_t & = \mathbf{s}_{t-1} + \mathbf{g}_t^2, \\
    \mathbf{w}_t & = \mathbf{w}_{t-1} - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}} \cdot \mathbf{g}_t.
\end{aligned}\end{split}$

其中 $ \epsilon$ 为一个为维持数值稳定性而添加的常数，确保我们不会除以零。就像在动量法中我们需要跟踪一个辅助变量一样，在AdaGrad算法中，我们允许每个坐标有单独的学习率。需要注意的是在 s~t~ 中累加平方梯度意味着s~t~ 基本上以线性速率额增长。具体代码如下：

```python
def init_adagrad_states(feature_dim):
    s_w = torch.zeros((feature_dim, 1))
    s_b = torch.zeros(1)
    return (s_w, s_b)

def adagrad(params, states, hyperparams):
    eps = 1e-6
    for p, s in zip(params, states):
        with torch.no_grad():
            s[:] += torch.square(p.grad)
            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)
        p.grad.data.zero_()
```

### 7、$RMSProp$ 算法

 `Adagrad` 算法的明显缺点是，它把梯度的平方类加成状态矢量s ~t~ ，由于缺乏规范化，没有约束力，s~t~ 持续增长，几乎上实在算法收敛时线性增长。解决的方法是：$ \mathbf{s}_t \leftarrow \gamma \mathbf{s}_{t-1} + (1-\gamma) \mathbf{g}_t^2$，同时保持其他部分不变就成为了 `RMSProp` 算法。具体方程如下：
$$
\begin{split}\begin{aligned}
    \mathbf{s}_t & \leftarrow \gamma \mathbf{s}_{t-1} + (1 - \gamma) \mathbf{g}_t^2, \\
    \mathbf{x}_t & \leftarrow \mathbf{x}_{t-1} - \frac{\eta}{\sqrt{\mathbf{s}_t + \epsilon}} \odot \mathbf{g}_t.
\end{aligned}\end{split}
$$
具体代码如下：

```python
def init_rmsprop_states(feature_dim):
    s_w = torch.zeros((feature_dim, 1))
    s_b = torch.zeros(1)
    return (s_w, s_b)

def rmsprop(params, states, hyperparams):
    gamma, eps = hyperparams['gamma'], 1e-6
    for p, s in zip(params, states):
        with torch.no_grad():
            s[:] = gamma * s + (1 - gamma) * torch.square(p.grad)
            p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps)
        p.grad.data.zero_()
```

- RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。
- RMSProp算法与动量法都使用泄漏平均值。但是，RMSProp算法使用该技术来调整按系数顺序的预处理器。
- 在实验中，学习率需要由实验者调度。
- 系数决定了在调整每坐标比例时历史记录的时长。

### 8、$Adadelta$ 算法

$Adadelta$ 是 $AdaGrad$  的另一种变体，主要区别在于前者减少了学习率适应坐标的数量。此外，广义上的 $AdaGrad$ 算法没有学习率，因为它使用了变量量本身作为未来变化的校准。简而言之，Adadelta使用两个状态变量，s~t~用于存储梯度二阶导数的泄露平均值，$ \Delta\mathbf{x}_t$用于存储模型本身中参数变化二阶导数的泄露平均值。具体方程如下：
$$
\begin{aligned}
    \mathbf{s}_t & = \rho \mathbf{s}_{t-1} + (1 - \rho) \mathbf{g}_t^2.
\end{aligned}
$$

$$
\begin{aligned}
    \Delta \mathbf{x}_t & = \rho \Delta\mathbf{x}_{t-1} + (1 - \rho) {\mathbf{g}_t'}^2,
\end{aligned}
$$

$$
\begin{split}\begin{aligned}
    \mathbf{g}_t' & = \frac{\sqrt{\Delta\mathbf{x}_{t-1} + \epsilon}}{\sqrt{{\mathbf{s}_t + \epsilon}}} \odot \mathbf{g}_t, \\
\end{aligned}\end{split}
$$

$$
\begin{split}\begin{aligned}
    \mathbf{x}_t  & = \mathbf{x}_{t-1} - \mathbf{g}_t'. \\
\end{aligned}\end{split}
$$

我们使用重新缩放的梯度g~t~ ^,^ 执行更新。.其中$\Delta \mathbf{x}_{t-1}$是重新缩放梯度的平方的泄漏平均值。我们将$\Delta \mathbf{x}_{0}$初始化为0，然后在每个步骤中使用 g~t~^,^更新它，

代码实现如下：

```python
%matplotlib inline
import torch
from d2l import torch as d2l


def init_adadelta_states(feature_dim):
    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)
    delta_w, delta_b = torch.zeros((feature_dim, 1)), torch.zeros(1)
    return ((s_w, delta_w), (s_b, delta_b))

def adadelta(params, states, hyperparams):
    rho, eps = hyperparams['rho'], 1e-5
    for p, (s, delta) in zip(params, states):
        with torch.no_grad():
            # In-placeupdatesvia[:]
            s[:] = rho * s + (1 - rho) * torch.square(p.grad)
            g = (torch.sqrt(delta + eps) / torch.sqrt(s + eps)) * p.grad
            p[:] -= g
            delta[:] = rho * delta + (1 - rho) * g * g
        p.grad.data.zero_()
```

### 9、$Adam$算法

$Adam$  算法汇总了前面的算法的优点。

- 随机梯度下降——在解决优化问题时比梯度下降更有效。
- 小批量随机梯度下降——可以充分利用GPU或者CPU的并行处理数据能力，更高效。
- 动量法——添加了一种机制，用于汇总过去梯度的历史以加速收敛。
- AdaGrad  算法——我们通过对每个坐标缩放实现高效计算的预处理器。
- RMSProp 算法——我们通过学习率的调整来分离每个坐标的缩放。

---

方程如下：

![image-20250725204620553](C:\Users\osquer\Desktop\typora图片\image-20250725204620553.png)

具体代码如下：

```python
%matplotlib inline
import torch
from d2l import torch as d2l


def init_adam_states(feature_dim):
    v_w, v_b = torch.zeros((feature_dim, 1)), torch.zeros(1)
    s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1)
    return ((v_w, s_w), (v_b, s_b))

def adam(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-6
    for p, (v, s) in zip(params, states):
        with torch.no_grad():
            v[:] = beta1 * v + (1 - beta1) * p.grad
            s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad)
            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)
                                                       + eps)
        p.grad.data.zero_()
    hyperparams['t'] += 1
```

- Adam算法将许多优化算法的功能结合到了相当强大的更新规则中。
- Adam算法在RMSProp算法基础上创建的，还在小批量的随机梯度上使用EWMA。
- 在估计动量和二次矩时，Adam算法使用偏差校正来调整缓慢的启动速度。

### 10、$Yogi$​算法

Adam算法也存在一些问题： 即使在凸环境下，当的二次矩估计值爆炸时，它可能无法收敛。作者建议在正式训练前，**用一个大批量数据**，先运行一次（或几次）梯度计算，得到更“可靠”的初始梯度统计量，再作为动量/方差估计的初始值。

![image-20250725205417840](C:\Users\osquer\Desktop\typora图片\image-20250725205417840.png)

```python
def yogi(params, states, hyperparams):
    beta1, beta2, eps = 0.9, 0.999, 1e-3
    for p, (v, s) in zip(params, states):
        with torch.no_grad():
            v[:] = beta1 * v + (1 - beta1) * p.grad
            s[:] = s + (1 - beta2) * torch.sign(
                torch.square(p.grad) - s) * torch.square(p.grad)
            v_bias_corr = v / (1 - beta1 ** hyperparams['t'])
            s_bias_corr = s / (1 - beta2 ** hyperparams['t'])
            p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr)
                                                       + eps)
        p.grad.data.zero_()
    hyperparams['t'] += 1

data_iter, feature_dim = d2l.get_data_ch11(batch_size=10)
d2l.train_ch11(yogi, init_adam_states(feature_dim),
               {'lr': 0.01, 't': 1}, data_iter, feature_dim);
```

- 对于具有显著差异的梯度，我们可能会遇到收敛性问题。我们可以通过使用更大的小批量或者切换到改进的估计值来修正它们。Yogi提供了这样的替代方案。

### 11、学习率调度器

#### 1、==综述==

1. 首先，学习率的大小很重要。如果它太大，优化就会发散；如果它太小，训练就会需要过长时间，或者我们最终只能得到次优的结果。
2. 其次，衰减速率同样很重要。如果学习率持续过高，我们可能最终会在最小值附近弹跳，从而无法达到最优解。
3. 另一个同样重要的方面是初始化。这既涉及参数最初的设置方式（，又关系到它们最初的演变方式。这被戏称为*预热*（warmup），即我们最初开始向着解决方案迈进的速度有多快。一开始的大步可能没有好处，特别是因为最初的参数集是随机的。最初的更新方向可能也是毫无意义的。
4. 虽然我们不可能涵盖所有类型的学习率调度器，但我们会尝试在下面简要概述常用的策略：多项式衰减和分段常数表。 此外，余弦学习率调度在实践中的一些问题上运行效果很好。 在某些问题上，最好在使用较高的学习率之前预热优化器。

---

#### 2、==下面是各种调度器，用于得到合适的学习率==

1. **单因子调度器**

   ![image-20250725211612794](C:\Users\osquer\Desktop\typora图片\image-20250725211612794.png)

```python
class FactorScheduler:
    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.1):
        self.factor = factor
        self.stop_factor_lr = stop_factor_lr
        self.base_lr = base_lr

    def __call__(self, num_update):
        self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)
        return self.base_lr

scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)
d2l.plot(torch.arange(50), [scheduler(t) for t in range(50)])
```

![image-20250725211655241](C:\Users\osquer\Desktop\typora图片\image-20250725211655241.png)

2. **多因子调度器**

<img src="C:\Users\osquer\Desktop\typora图片\image-20250725212913226.png" alt="image-20250725212913226" style="zoom:80%;" />

调度器并不知道「你正在第几个 epoch」，它只是每次你调用 scheduler.step()，就内部计数 +1。一切靠你在训练循环中按正确时机调用它。

```python
net = net_fn()
trainer = torch.optim.SGD(net.parameters(), lr=0.5)
scheduler = lr_scheduler.MultiStepLR(trainer, milestones=[15, 30], gamma=0.5)

def get_lr(trainer, scheduler):
    lr = scheduler.get_last_lr()[0]
    trainer.step()
    scheduler.step()
    return lr

d2l.plot(torch.arange(num_epochs), [get_lr(trainer, scheduler)
                                  for t in range(num_epochs)])
```

![../_images/output_lr-scheduler_1dfeb6_98_0.svg](https://zh.d2l.ai/_images/output_lr-scheduler_1dfeb6_98_0.svg)

3. **余弦调度器**（Cosine Annealing Scheduler）

学习率从初始值逐步 **沿余弦曲线下降**，最终接近 0。

```
初始 lr ↓          ⎯⎯⎯⎯⎯
                ／        ＼
              ／            ＼
            ／                ＼
末尾 lr →                 ⎯⎯⎯⎯⎯

```

![image-20250725213350658](C:\Users\osquer\Desktop\typora图片\image-20250725213350658.png)



```python
class CosineScheduler:
    def __init__(self, max_update, base_lr=0.01, final_lr=0,
               warmup_steps=0, warmup_begin_lr=0):
        self.base_lr_orig = base_lr
        self.max_update = max_update
        self.final_lr = final_lr
        self.warmup_steps = warmup_steps
        self.warmup_begin_lr = warmup_begin_lr
        self.max_steps = self.max_update - self.warmup_steps

    def get_warmup_lr(self, epoch):
        increase = (self.base_lr_orig - self.warmup_begin_lr) \
                       * float(epoch) / float(self.warmup_steps)
        return self.warmup_begin_lr + increase

    def __call__(self, epoch):
        if epoch < self.warmup_steps:
            return self.get_warmup_lr(epoch)
        if epoch <= self.max_update:
            self.base_lr = self.final_lr + (
                self.base_lr_orig - self.final_lr) * (1 + math.cos(
                math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2
        return self.base_lr

scheduler = CosineScheduler(max_update=20, base_lr=0.3, final_lr=0.01)
d2l.plot(torch.arange(num_epochs), [scheduler(t) for t in range(num_epochs)])
```

4. **预热**

预热（Warm-up）指的是在训练初期，逐步从较小的学习率增加到设定的初始学习率。它不是直接使用目标学习率，而是像这样慢慢升上去：

```python
学习率曲线：
         /‾‾‾‾‾‾‾‾‾‾‾‾
        /
       /
______/

```

它的整体目标是：

- **前若干步先预热**：学习率从很小线性增长到 base_lr。
- **中后期逐渐减小学习率**：使用余弦函数平滑地从 base_lr 减到 final_lr。

---

```python
class CosineScheduler:
    def __init__(self, 
                 max_update,         # 总的训练轮数（或步数）
                 base_lr=0.01,       # 初始学习率（预热后达到的）
                 final_lr=0,         # 最终学习率（训练结束后达到的）
                 warmup_steps=0,     # 预热的轮数
                 warmup_begin_lr=0): # 预热开始时的学习率
```

举例：

- 如果 `warmup_steps=5`，`base_lr=0.3`，`final_lr=0.01`，`max_update=20`：
  - 前5个 epoch：线性预热从 0 → 0.3
  - 第6~20个 epoch：从 0.3 通过余弦退火平滑地减小至 0.01

---



1️⃣ 计算预热阶段学习率（线性增长）

```python
def get_warmup_lr(self, epoch):
    increase = (self.base_lr_orig - self.warmup_begin_lr) * float(epoch) / float(self.warmup_steps)
    return self.warmup_begin_lr + increase
```

这段代码的含义是：

- 第 `epoch` 步的学习率 = 初始值 + 线性增长的比例
- 比如 warmup 从 0 到 0.3，共 5 步，那么第 3 步的学习率大约是 `0 + (0.3 - 0) * 3/5 = 0.18`

------



2️⃣ 正式退火阶段（余弦函数控制学习率下降）

```python
if epoch <= self.max_update:
    self.base_lr = self.final_lr + (
        self.base_lr_orig - self.final_lr) * 
        (1 + math.cos(math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2
```



------

✅ 优点总结

| 特性               | 说明                                                |
| ------------------ | --------------------------------------------------- |
| 🚀 Warm-up          | 初始训练不稳定，避免过大梯度引起发散                |
| 🌊 Cosine Annealing | 平滑收敛，避免震荡，适合 fine-tuning                |
| 🔧 参数灵活         | 可设置最大/最小学习率、warmup长度等                 |
| 🎯 精度提升         | 实际应用中，常用于 ResNet、Transformer 等大模型训练 |

## 4、计算性能

### 1、编译器和解释器

==<  命令式编程  >==  命令式编程是一种按照**指令一步步改变程序状态**的编程风格，程序员告诉计算机 **怎么做**、按什么步骤执行。

```python
total = 0
for i in range(10):
    total += i
print(total)

```

==<  符号式编程  >==  符号式编程强调用**符号表示抽象问题**，程序结构就是数据结构，程序**可以操作自身的代码结构**，更侧重“表达”和“重写”而不是执行指令。因为符号式编程是将代码**作为表达式结构**来处理，编译器可以“看到”整个表达式的结构，所以就有空间做**很多自动优化**。

```python
(define (square x) (* x x))
(square 5)

```

命令式（解释型）编程和符号式编程的区别如下：

- 命令式编程更容易使用。在Python中，命令式编程的大部分代码都是简单易懂的。命令式编程也更容易调试，这是因为无论是获取和打印所有的中间变量值，或者使用Python的内置调试工具都更加简单；
- 符号式编程运行效率更高，更易于移植。符号式编程更容易在编译期间优化代码，同时还能够将程序移植到与Python无关的格式中，从而允许程序在非Python环境中运行，避免了任何潜在的与Python解释器相关的性能问题。



```python
net = torch.jit.script(net)
net(x)

```

上方的代码可以将命令式模型转换为符号式模式。

==<  解释器  >==  程序运行时，**逐行翻译+立即执行**源码，边读边干

==<  编译器  >==  在程序运行前，**一次性把源码翻译成机器码/字节码**，之后直接运行已编译好的结果。

### 2、异步计算

`pytorch` 的GPU操作默认是异步执行的 ，把数据送入GPU之后，python会立即收回控制权，此时可能数据还没有算完。

广义上说，PyTorch有一个用于与用户直接交互的前端（例如通过Python），还有一个由系统用来执行计算的后端。PyTorch 的 Python 层**只负责构建计算任务并发出指令，真正的计算任务是由底层的 C++/CUDA 后端完成的，并且默认是异步执行的**。当你执行 `z = x + y` 这样的操作时，Python 并没有真正去“计算”加法；它只是构造了一个“加法任务”并传给 C++ 层，后者再决定怎么交给 CUDA 执行；一直到你真正需要这个结果（比如 `print(z)`），**PyTorch 才同步等待后端完成任务**。（==“前后端解耦”==）

### 3、硬件基础

<u>计算机由以下关键部件组成：</u>

- 一个处理器（也被称为CPU），它除了能够运行操作系统和许多其他功能之外，还能够执行给定的程序。它通常由个或更多个核心组成；
- 内存（随机访问存储，RAM）用于存储和检索计算结果，如权重向量和激活参数，以及训练数据；
- 一个或多个以太网连接，速度从1GB/s到100GB/s不等。在高端服务器上可能用到更高级的互连；
- 高速扩展总线（PCIe）用于系统连接一个或多个GPU。服务器最多有个加速卡，通常以更高级的拓扑方式连接，而桌面系统则有个或个加速卡，具体取决于用户的预算和电源负载的大小；
- 持久性存储设备，如磁盘驱动器、固态驱动器，在许多情况下使用高速扩展总线连接。它为系统需要的训练数据和中间检查点需要的存储提供了足够的传输速度。



==<  缓存  >==

- **一级缓存**是应对高内存带宽要求的第一道防线。一级缓存很小（常见的大小可能是32-64KB），内容通常分为数据和指令。当数据在一级缓存中被找到时，其访问速度非常快，如果没有在那里找到，搜索将沿着缓存层次结构向下寻找。
- **二级缓存**是下一站。根据架构设计和处理器大小的不同，它们可能是独占的也可能是共享的。即它们可能只能由给定的核心访问，或者在多个核心之间共享。二级缓存比一级缓存大（通常每个核心256-512KB），而速度也更慢。此外，我们首先需要检查以确定数据不在一级缓存中，才会访问二级缓存中的内容，这会增加少量的额外延迟。
- **三级缓存**在多个核之间共享，并且可以非常大。AMD的EPYC 3服务器的CPU在多个芯片上拥有高达256MB的高速缓存。更常见的数字在4-8MB范围内。



==<  深度学习的互联方式  >==  ![image-20250727184633244](C:\Users\osquer\Desktop\typora图片\image-20250727184633244.png)

## 5、自然语言处理

### 1、跳元模型和连续词袋模型

 **跳元模型（Skip-gram）** 和 **连续词袋模型（CBOW, Continuous Bag of Words）** 两者都是 **Word2Vec** 框架下的两种词向量训练方法，核心目标都是：

> 根据词在上下文的分布规律学习词的稠密向量表示，使得语义相近的词在向量空间中距离更近。

------

#### 2️⃣ 核心思想

| 模型          | 核心任务                          |
| ------------- | --------------------------------- |
| **CBOW**      | 用 **上下文词** 来预测 **中心词** |
| **Skip-gram** | 用 **中心词** 来预测 **上下文词** |

------

#### 3️⃣ 输入输出结构

**CBOW**

- **输入**：上下文中的多个词（窗口大小 `m`）
- **输出**：窗口中间的那个中心词
- **例子**（窗口大小=2）：
  句子 `The quick brown fox jumps`
  预测 `"brown"` 的输入是 `["The", "quick", "fox", "jumps"]`

**Skip-gram**

- **输入**：一个中心词
- **输出**：它的上下文词
- **例子**（窗口大小=2）：
  输入 `"brown"` → 输出 `["The", "quick", "fox", "jumps"]`

------

#### 4️⃣ 模型结构（训练过程）

CBOW 过程

1. 对输入的上下文词做 **词向量查表** → 得到多组词向量
2. 求平均（或加和） → 得到上下文向量
3. 经过全连接层（或 softmax）预测中心词
4. 用交叉熵损失更新词向量参数

Skip-gram 过程

1. 对输入的中心词做 **词向量查表**
2. 将中心词向量送入模型，分别预测每个上下文词
3. 多个输出词的损失求和
4. 反向传播更新参数

------

#### 5️⃣ 对比总结

| 对比维度         | CBOW                   | Skip-gram              |
| ---------------- | ---------------------- | ---------------------- |
| **预测方向**     | 上下文 → 中心词        | 中心词 → 上下文        |
| **适合数据规模** | 大语料，低频词训练不佳 | 小语料也能表现好       |
| **训练速度**     | 快（一次预测一个词）   | 慢（一次预测多个词）   |
| **低频词效果**   | 较差                   | 更好                   |
| **优点**         | 训练快、对高频词效果好 | 能更好捕捉罕见词的语义 |
| **缺点**         | 低频词语义学习差       | 训练时间长             |



### 2、负采样和分层 `softmax`

好，那我帮你系统整理一下 **负采样（Negative Sampling）** 和 **层序 Softmax（Hierarchical Softmax）**，这两个是 Word2Vec 里常用的 **softmax 加速技巧**。

------

#### 1️⃣ 背景问题：为什么要加速？

在 Word2Vec（Skip-gram 或 CBOW）里，输出层通常是一个 **softmax 分类器**：

问题：

- 词表大小 VV 可以是几十万甚至上百万
- 每次计算梯度都要遍历整个词表 → **非常慢**

于是就有了 **近似 softmax** 的两种主流方法：

- 负采样（Negative Sampling）
- 层序 softmax（Hierarchical Softmax）

------

#### 2️⃣ 负采样（Negative Sampling）

💡 核心思想

不去更新整个词表的参数，而是：

1. 更新**正样本**（目标词）的参数
2. 再随机挑选 k 个**负样本**（非目标词）更新参数

这样每次只计算 k+1个词，而不是 V 个。



------

#### 3️⃣ 层序 Softmax（Hierarchical Softmax）

💡 核心思想

用一棵 **霍夫曼树（Huffman Tree）** 代替扁平化的 softmax 分类器。

- 词表中每个词是叶子节点

- 每个内部节点是一个二分类器

- 词的概率 = 从根到该词叶子的路径上所有二分类器概率的乘积

  

------

#### 4️⃣ 对比总结

| 特性             | 负采样                | 层序 Softmax         |
| ---------------- | --------------------- | -------------------- |
| **时间复杂度**   | O(k)                  | O(log⁡V)              |
| **实现难度**     | 简单                  | 较复杂               |
| **低频词效果**   | 好                    | 一般                 |
| **适合场景**     | 词表大、需要稀疏更新  | 词表特别大（百万+）  |
| **是否全局更新** | 否（只更新 k+1 个词） | 否（只更新路径节点） |

---

#### 5️⃣ 小记忆口诀

> - **负采样**：扔掉大部分词，只和少数“假词”比
> - **层序 Softmax**：走一条树路径，省得遍历全词表



## 6、数学基础

### 1、张量

==张量可以存储数据，并在gpu上面进行计算，并支持自动微分。==

#### 📦 1. 张量创建

```python
import torch

# 零 / 一
torch.zeros(2, 3)                 # 2x3 全零张量
torch.ones((4, 5))               # 4x5 全一张量

# 随机
torch.rand(3, 3)                 # 均匀分布[0,1)
torch.randn(3, 3)                # 标准正态分布 N(0,1)

# 常数/指定值
torch.full((2, 2), 7.5)          # 全为7.5的张量

# 从列表创建
torch.tensor([[1, 2], [3, 4]])   # 显式创建

# 类似 numpy 的 linspace/eye
torch.linspace(0, 1, steps=5)    # 均分 0~1 的 5 个数
torch.eye(3)                     # 单位矩阵
```

------

#### 🧮 2. 基础数学运算

```python
a = torch.tensor([1.0, 2.0, 3.0])
b = torch.tensor([4.0, 5.0, 6.0])

# 加减乘除
a + b
a - b
a * b
a / b

# 指数、平方根、log
torch.exp(a)
torch.sqrt(b)
torch.log(b)

# 点积、矩阵乘法
torch.dot(a, b)                    # 向量点积
A = torch.randn(2, 3)
B = torch.randn(3, 4)
torch.matmul(A, B)                # 矩阵乘法
```

------

#### 🔄 3. 维度变换操作

```python
x = torch.randn(2, 3)

x.view(3, 2)                      # 改变 shape（连续张量）
x.reshape(3, 2)                   # 改变 shape（更通用）

x.T                               # 转置（2D）
x.permute(1, 0)                   # 更通用的维度重排

x.unsqueeze(0)                    # 增加维度 [2,3] → [1,2,3]
x.unsqueeze(-1)                   # 增加最后一维

x.squeeze()                       # 去掉所有维度为1的轴
```

------

#### 🧱 4. 拼接与分割

```python
a = torch.randn(2, 3)
b = torch.randn(2, 3)

torch.cat((a, b), dim=0)          # 沿 dim=0 拼接 → [4,3]
torch.stack([a, b], dim=0)        # 添加新维度 → [2,2,3]

# 拆分
x = torch.randn(4, 6)
torch.chunk(x, 2, dim=0)          # 沿 dim=0 分成2块
torch.split(x, 3, dim=1)          # 每块3列
```

------

#### 🎯 5. 索引 & 条件操作

```python
x = torch.tensor([[1, 2], [3, 4]])

x[0, 1]                            # 获取指定位置
x[:, 1]                            # 获取第2列
x[x > 2]                           # 条件筛选

torch.where(x > 2, x, torch.zeros_like(x))  # 条件选择
```

------

#### 🔁 6. 广播（Broadcasting）

```python
x = torch.tensor([[1.0], [2.0]])    # shape [2,1]
y = torch.tensor([10.0, 20.0])      # shape [2]

# 会自动扩展为 [2,2]
x * y                              # 广播乘法
```

------

#### 🧠 7. 聚合运算

```python
x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])

x.sum()                            # 所有元素求和
x.mean(dim=0)                      # 每列均值
x.max(dim=1)                       # 每行最大值和索引
x.argmin()                         # 最小值索引
```

------

#### 🧷 8. 数据类型与设备管理

```python
x.float()                          # 转 float32
x.double()                         # 转 float64
x.int()                            # 转 int32

x.to("cuda")                       # 送到 GPU
x.to(torch.device("cpu"))         # 回到 CPU
```

------

#### 📐 9. 维度命名（高级特性）

```python
# 需要启用
torch.set_default_tensor_type('torch.FloatTensor')

x = torch.randn((2, 3), names=('N', 'C'))
x = x.rename(None)                 # 移除名字
```

------

#### 🔒 10. 原地操作（以 _ 结尾）

```python
x = torch.tensor([1.0, 2.0])
x.add_(3.0)                        # 原地加法 x = x + 3
x.zero_()                          # 原地归零
```

------

#### ✅ 总结建议

| 类型     | 推荐操作                          |
| -------- | --------------------------------- |
| 创建张量 | `torch.tensor / rand / zeros`     |
| 运算     | `+ - * /`, `matmul`, `dot`        |
| 维度变换 | `reshape`, `permute`, `unsqueeze` |
| 广播     | 自动完成（理解右对齐即可）        |
| GPU 支持 | `.to("cuda")` 或 `device="cuda"`  |

### 2、广播机制

1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；
2. 对生成的数组执行按元素操作。

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```

---

### 3、线性代数

```python
A.sum(axis=1)
A.sum(axis=1,keepdims=True)   # 保持维度
A.cumsum(axis=1)  #沿着该轴计算累计和，并且不改变张量shape
A * B  # 逐元素乘法,即对应位置相乘再相加
A @ B  # 元素点积
torch.mv(A,B)  # 矩阵-向量积
torch.mm(A,B)  # 矩阵乘法，适用于二维以下的矩阵乘法
```

![image-20250811212504070](C:\Users\osquer\Desktop\typora图片\image-20250811212504070.png)

---



线性代数中最有用的一些运算符是*范数*（norm）。 非正式地说，向量的*范数*是表示一个向量有多大。 这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。

![image-20250811213436334](C:\Users\osquer\Desktop\typora图片\image-20250811213436334.png)

 $\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.$ 为 L1 范数 ，$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$ 为L2范数，

![image-20250811213612962](C:\Users\osquer\Desktop\typora图片\image-20250811213612962.png) 

**范数和目标**

在深度学习中，我们经常试图解决优化问题： *最大化*分配给观测数据的概率; *最小化*预测和真实观测之间的距离。 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数



## 8、模型度量方法（损失函数）

==*损失函数*（loss function）能够量化目标的*实际*值与*预测*值之间的差距。==



### 1. 均方误差 (MSE) 损失

- **使用场景**：主要用于回归任务，目标是预测连续值，例如房价预测或温度预测。

- **选择原因**：MSE因其平方操作对较大误差施加更大惩罚，适用于需要特别避免大偏差的情景。此外，其可微性支持PyTorch中的梯度优化。

- **特征**：

  - 测量预测值与实际值平均平方差。

  - 由于二次性质，对异常值敏感。

  - 在PyTorch中通过 `torch.nn.MSELoss()` 实现。

  - 公式：  
    $$
    \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    $$

  ```
  criterion = nn.MSELoss()
  loss = criterion(y_pred, y)
  ```

  

### 2. 均绝对误差 (MAE) 损失

- **使用场景**：适用于稳健性要求较高的回归任务，例如金融预测中对异常值的容忍。

- **选择原因**：MAE对所有误差的惩罚线性相关，较少受极端值影响，适合数据分布不均匀或存在噪声的情况。

- **特征**：

  - 计算预测值与实际值绝对差的平均值。

  - 对异常值鲁棒性强，但梯度信息较少。

  - 在PyTorch中通过 `torch.nn.L1Loss()` 实现。

  - 公式：  
    $$
    \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
    $$

  ```
  criterion = nn.L1Loss()
  loss = criterion(y_pred, y)
  ```

  

### 3. 交叉熵损失 (Cross-Entropy Loss)

- **使用场景**：广泛用于分类任务，包括二分类（sigmoid激活）和多分类（softmax激活），如图像分类或自然语言处理。

- **选择原因**：交叉熵损失直接优化分类问题的对数似然，适用于概率输出模型，能有效区分正确类别与错误类别。

- **特征**：

  - 结合softmax或sigmoid与负对数似然，衡量预测概率分布与真实分布的差异。

  - 对置信度低的预测施加较大惩罚。

  - 在PyTorch中通过 `torch.nn.CrossEntropyLoss()` 实现（包含softmax）。

  - 公式（二分类简化）：  
    $$
    \text{CE} = - \frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
    $$

```
criterion = nn.CrossEntropyLoss()
loss = criterion(y_pred, y)
```



### 4. 二元交叉熵损失 (Binary Cross-Entropy Loss)

- **使用场景**：专用于二分类任务，例如垃圾邮件检测或疾病诊断。

- **选择原因**：针对二值输出优化，适用于sigmoid激活后的概率预测，计算效率高。

- **特征**：

  - 针对单个二元分类器，评估正类和负类的对数损失。

  - 需与 `torch.nn.Sigmoid()` 结合使用，或直接用 `torch.nn.BCELoss()`。

  - 公式：  
    $$
    \text{BCE} = - \frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
    $$

```
criterion = nn.BCELoss()
loss = criterion(y_pred, y)
```



### 5. 枢纽损失 (Huber Loss)

- **使用场景**：在回归任务中，需平衡MSE对异常值的敏感性和MAE的鲁棒性，例如机器人控制或时间序列预测。

- **选择原因**：Huber损失在误差较小时接近MSE，在误差较大时接近MAE，兼顾稳定性与梯度平滑。

- **特征**：

  - 引入阈值参数（默认1），控制线性与二次区域的切换。

  - 对异常值较鲁棒，梯度平滑。

  - 在PyTorch中通过 `torch.nn.HuberLoss()` 实现。

  - 公式：  
    $$
    \text{Huber}(x) = 
    \begin{cases} 
    \frac{1}{2} x^2 & \text{if } |x| \leq \delta \\
    \delta |x| - \frac{1}{2} \delta^2 & \text{if } |x| > \delta 
    \end{cases}
    $$

### 总结与选择指导

- **回归任务**：MSE适合数据平滑，MAE或Huber适合异常值较多；Huber为折中选择。
- **分类任务**：交叉熵适用于平衡数据集，二元交叉熵专为二分类，Focal Loss针对不平衡数据。
- **特点对比**：MSE和交叉熵对梯度敏感，MAE和Huber更鲁棒，Focal Loss强调困难样本。
- 在PyTorch中，损失函数通过 `torch.nn` 模块调用，需根据任务需求选择并与模型输出格式匹配（例如，`CrossEntropyLoss` 需原始logits）

---

## 9、参数初始化

参数初始化是深度学习模型训练的重要步骤，直接影响收敛速度和最终性能。以下是对 PyTorch 中常用参数初始化方法（以 `net[0].weight.data.normal_(0, 0.01)` 和 `net[0].bias.data.fill_(0)` 为例）的总结，包括相关方法、应用场景及原因，保持专业、清晰的语言结构。

### 1. 常用参数初始化方法

#### a. 正态分布初始化 (`normal_`)

- **方法**: `tensor.normal_(mean, std)` 或 `tensor.data.normal_(mean, std)`
  - 例如: `net[0].weight.data.normal_(0, 0.01)` 将 `weight` 初始化为均值为 0、标准差为 0.01 的正态分布。
- **应用场景**:
  - 适用于大多数全连接层和卷积层的权重初始化，尤其在深层网络中。
  - 常见于激活函数为 ReLU 或其变种的网络（如 ResNet、Transformer），或需要小初始权重以避免梯度问题。
- **原因**:
  - 小标准差（如 0.01）防止初始权重过大，减少梯度爆炸或消失风险。
  - 均值 0 确保对称性，便于梯度传播。
  - Xavier/Glorot 初始化（`nn.init.xavier_normal_`）是其改进版，基于输入输出维度自适应标准差。

#### b. 常数填充 (`fill_`)

- **方法**: `tensor.fill_(value)` 或 `tensor.data.fill_(value)`
  - 例如: `net[0].bias.data.fill_(0)` 将偏置初始化为 0。
- **应用场景**:
  - 偏置项（bias）的标准初始化，特别是在 ReLU 激活前。
  - 某些情况下用于特定层（如批量归一化层的偏置），但需谨慎。
- **原因**:
  - 偏置初始化为 0 不会影响网络对称性，且与 ReLU（输出非负）兼容。
  - 避免初始输出偏移，保持梯度流动平稳。

#### c. 均匀分布初始化 (`uniform_`)

- **方法**: `tensor.uniform_(low, high)` 或 `tensor.data.uniform_(low, high)`
  - 例如: `nn.init.uniform_(tensor, a=0, b=0.01)`
- **应用场景**:
  - 适用于权重初始化，特别是在浅层网络或 sigmoid/tanh 激活函数的场景。
  - Xavier 均匀初始化 (`nn.init.xavier_uniform_`) 基于输入输出维度调整范围。
- **原因**:
  - 均匀分布提供多样性，防止权重过于集中。
  - 范围需根据激活函数选择（如 [-0.1, 0.1] 避免饱和）。

#### d. Xavier/Glorot 初始化

- **方法**: `nn.init.xavier_normal_(tensor)` 或 `nn.init.xavier_uniform_(tensor)`
- **应用场景**:
  - 适用于深层网络，尤其是使用 sigmoid 或 tanh 激活的模型。
  - 常用于 CNN 和 RNN 的权重初始化。
- **原因**:
  - 根据输入维度 \(n_{\text{in}}\) 和输出维度 \(n_{\text{out}}\)，标准差为 \(\sqrt{\frac{2}{n_{\text{in}} + n_{\text{out}}}}\)，平衡前向和反向信号。

#### e. He 初始化

- **方法**: `nn.init.kaiming_normal_(tensor, mode='fan_out', nonlinearity='relu')`
- **应用场景**:
  - 专为 ReLU 及其变种激活函数设计的网络（如现代 CNN，如 VGG、ResNet）。
- **原因**:
  - 标准差为 \(\sqrt{\frac{2}{n_{\text{in}}}}\)，适配 ReLU 的单侧激活特性，防止梯度消失。

#### f. 预训练初始化

- **方法**: 加载预训练模型权重，例如 `model.load_state_dict(torch.load('pretrained.pth'))`
- **应用场景**:
  - 迁移学习场景，如使用 ImageNet 预训练的 ResNet 模型。
- **原因**:
  - 利用预训练权重加速收敛，改善小数据集性能。

### 2. 应用场景总结

- **浅层网络**: 均匀初始化或小正态初始化（std=0.01）即可，简单有效。
- **深层网络**:
  - ReLU 激活: He 初始化（`kaiming_normal_`）最佳。
  - Sigmoid/Tanh 激活: Xavier 初始化（`xavier_normal_` 或 `xavier_uniform_`）更合适。
- **偏置**: 通常初始化为 0，除非特定需求（如批量归一化层的偏置可能初始化为小值）。
- **迁移学习**: 优先使用预训练权重，微调特定层。
- **特殊情况**: 某些模型（如 LSTM）可能需要正交初始化 (`nn.init.orthogonal_`) 以保持梯度流动。

### 3. 注意事项

- **激活函数匹配**: 初始化方法需与激活函数匹配，否则可能导致梯度问题（例如，ReLU 需 He 初始化，sigmoid 需 Xavier）。
- **数据尺度**: 初始化范围应与输入数据尺度一致，避免过大或过小。
- **in-place 操作**: 方法如 `normal_`, `fill_` 直接修改 `data`，需谨慎使用。
- **PyTorch 集成**: 使用 `nn.Module` 的参数时，可通过 `nn.init` 模块或直接操作 `.weight.data` 和 `.bias.data`。

### 结论

参数初始化如 `net[0].weight.data.normal_(0, 0.01)` 和 `net[0].bias.data.fill_(0)` 是标准做法，分别适配权重的小随机初始化和偏置的零初始化。选择方法需根据网络深度、激活函数和任务需求，Xavier 和 He 初始化为深层网络的优选，预训练初始化则适用于迁移学习。合理初始化可显著提升训练效率和模型性能。

## 10、经典网络层

### 1、线性神经网络

![image-20250813104626628](C:\Users\osquer\Desktop\typora图片\image-20250813104626628.png)

由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。 我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换（ [图3.1.2](https://zh.d2l.ai/chapter_linear-networks/linear-regression.html#fig-single-neuron)中的输出层） 称为*全连接层*（fully-connected layer）或称为*稠密层*（dense layer）。 

当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。

### 2、Dropout层 

#### 1、基本概况（[返回原连接](#2、dropout 丢弃法)）

- **训练阶段**: Dropout 以概率 \( p \)（例如 0.5）随机将神经网络中某些神经元的输出置为 0，同时将保留神经元的输出乘以$ \frac{1}{1-p}$（例如 2），以保持期望输出不变。每次前向传播，丢弃的神经元集合是随机的。

- **测试阶段**: Dropout 被禁用，使用完整模型，不进行丢弃或缩放。

- **目标**: 通过随机丢弃，模拟多个子网络的训练，增强模型泛化能力，防止过拟合。

-  应用场景：

   - 适用于深层网络，尤其是卷积神经网络（CNN）和循环神经网络（RNN），如 ResNet、LSTM。
   - 特别适合数据量不足或模型过参数化的情况。

- **代码示例**:

  ```python
  import torch
  import torch.nn as nn
  
  model = nn.Sequential(
      nn.Linear(10, 5),
      nn.Dropout(p=0.5),  # 50% dropout 率
      nn.ReLU(),
      nn.Linear(5, 2)
  )
  ```

#### 2. 您的疑问解析

- **问题核心**: 您提到“当丢弃一些神经元后，此时的隐藏层与权重相乘，但下一次丢弃时，隐藏层与之前不同，那学习的权重不就效果没那么好了吗？”
  - **观察**: 每次训练迭代中，Dropout 随机丢弃不同神经元，导致隐藏层输出的激活值（即与权重相乘的输入）在不同批次或迭代中变化。这可能让人疑惑：如果权重是基于当前隐藏层学习的，下次隐藏层变化后，权重是否仍适用？
  - **潜在担忧**: 您可能担心权重学习变得不稳定，因为它们是为特定丢弃模式优化的，而每次丢弃模式不同。

#### 3. 为什么权重学习不会“效果变差”

- **Dropout 的正则化效应**:
  - Dropout 迫使网络在训练时学习更鲁棒的权重。每次丢弃不同神经元，模型必须依靠剩余神经元的组合来完成任务。这鼓励权重分布更均匀，不依赖特定神经元，类似于集成学习（ensemble learning）的效果。
  - 例如，假设隐藏层有 10 个神经元，\( p = 0.5 \)，每次可能丢弃 5 个。网络学会了多种子网络的权重配置，最终权重适应了多种丢弃模式。

- **权重更新的适应性**:
  - 权重通过梯度下降优化，基于整个训练过程的平均梯度更新。Dropout 引入的随机性被平均化，权重逐渐调整为适合各种丢弃配置。
  - 数学上，Dropout 的期望输出保持不变（\(\mathbb{E}[h_{\text{dropout}}] = h\)，其中 \( h \) 是原始激活），这确保权重学习的目标一致。

- **隐藏层变化的意义**:
  - 隐藏层输出变化是 Dropout 设计的意图。每次不同的丢弃模式相当于训练一个新的子网络，权重通过多次迭代学习这些子网络的共同模式。
  - 这种变化不会使权重“失效”，而是使权重更具泛化性，因为它们必须适应随机缺失的输入。

#### 4. 为什么不会影响学习效果

- **鲁棒性提升**:
  - 如果权重过于依赖特定神经元，Dropout 的随机丢弃会惩罚这种依赖，迫使网络学习更分散的表示。这提高了模型对噪声或数据变化的鲁棒性。
  - 例如，在垃圾邮件分类中，如果权重过于依赖“尼日利亚”一词，丢弃后网络必须依靠其他特征（如“西联汇款”），最终学习两者的组合效应。

- **测试时的完整性**:
  - 测试时关闭 Dropout，使用所有神经元，权重已通过训练适应了多种子网络的平均行为。缩放因子 \(\frac{1}{1-p}\) 确保训练和测试的期望输出一致，避免偏差。

- **梯度平均效应**:
  - 反向传播的梯度是基于当前批次的损失计算的。Dropout 随机性被多次迭代平滑，权重更新反映了整体趋势，而非单一丢弃模式。

#### 5. 澄清困惑

- **“学习权重效果没那么好”**:
  - 短期看，特定迭代的权重可能不完美匹配当前丢弃模式，但长期看，权重通过多次随机丢弃学习到稳健解。
  - 这是 Dropout 的设计目标：牺牲短期一致性，换取长期泛化能力。
- **“隐藏层与之前不同”**:
  - 不同是正常的，Dropout 利用这种变化迫使网络分散学习，而不是依赖固定结构。

#### 6. 直观类比

- 想象一个足球队：训练时随机让部分球员休息（Dropout），迫使其他球员学会协作；比赛时（测试）所有球员上场，团队已适应多种配置，表现更稳定。权重就像球员的技能，随机休息让技能更全面。

#### 7. 实践验证

- **实验建议**: 尝试一个简单网络，比较有/无 Dropout 的训练过程。观察权重变化和测试准确率，验证随机性如何提升泛化。

#### 结论

Dropout 随机丢弃神经元导致隐藏层变化，但不会使权重学习效果变差。相反，这种变化通过正则化迫使权重适应多种子网络，增强泛化性。测试时关闭 Dropout 利用完整模型，权重已优化为稳健解。理解这一机制有助于优化 Dropout 率 \( p \) 和网络设计。



### 3、卷积层

#### 1、**卷积层的特点：**

1. *平移不变性*（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。
2. *局部性*（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。

卷积层会将输入和核矩阵进行交叉相关，奖赏偏移后得到输出，其中核矩阵和偏移和可学习参数，核矩阵的大小为超参数。

#### 2、特征映射

- **定义**:
  - 卷积层的输出通常被称为特征映射，因为它将输入数据（例如图像）通过卷积操作转换为下一层的空间维度表示。每个特征映射对应于一个卷积核（filter），提取输入中的特定特征（如边缘、纹理）。
- **作为转换器**:
  - 特征映射可以将输入的空间信息（宽度、高度）和通道信息映射到新的表示，保留局部模式，同时减少全局冗余。

#### 3、感受野

- **定义**:
  - 在 CNN 的某一层中，任意输出元素 \( y[i, j] \) 的感受野是指在前向传播期间，==所有可能影响其计算的输入元素集合==。这些输入元素通常来自所有先前层，覆盖一个局部的空间区域。
  - 感受野的大小由卷积核尺寸、层数、步幅和池化操作共同决定。
- **直观理解**:
  - 感受野描述了网络中某一神经元“看到”的输入区域。例如，在第一层卷积，感受野等于滤波器大小；在深层，感受野扩展到输入的更大区域。
- **示例**:
  - 对于一个$  3 \times 3  $滤波器，第一层的输出元素受输入中 $  3 \times 3  $ 区域影响。
  - 如果第二层再次应用$  3 \times 3  $ 滤波器，感受野扩展到$  5 \times 5  $（因为每个输出元素依赖前一层 $  3 \times 3  $的输出，而前一层输出又依赖输入$  3 \times 3  $。

#### 4、卷积权重

卷积核与输入的乘积大小代表了这块区域的匹配程度，但需要注意的是卷积中的“匹配”不是靠大小，而是靠方向，乘积结果只有在方向匹配的情况下，数值才有意义

#### 5、卷积输入和输出的联系

$$
W=(W+Padding*2-kernel Size+strides)//strides
$$

当 $ Padding*2-kernel Size+1=0$​ 时，输入和输出的图像大小一致。

#### 6、多输入和多输出通道



![image-20250815175816473](C:\Users\osquer\Desktop\typora图片\image-20250815175816473.png)

* 输出通道数是卷积层的超参数
* 每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果
* 每个输出通道有独立的三维卷积核： $Cin \times kernel\ size \times kernel \ size$​
* 此外， $  1 \times 1  $​ 卷积可以在不改变空间特征的情况下，  变换输出通道数。

### 4、汇聚层（池化层）

在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。 这意味着汇聚层的输出通道数与输入通道数相同。
==注意点==:对于池化层，如果不显著设定stride，那么stride就会和池化窗口的size相等，对于卷积层，如果不显著设定stride，那么stride的size会默认为1。

- 对于给定输入元素，最大汇聚层会输出该窗口内的最大值，平均汇聚层会输出该窗口内的平均值。
- 汇聚层的主要优点之一是减轻卷积层对位置的过度敏感。
- 我们可以指定汇聚层的填充和步幅。
- 使用最大汇聚层以及大于1的步幅，可减少空间维度（如高度和宽度）。
- 汇聚层的输出通道数与输入通道数相同。
- 没有可学习的参数



### 5、BatchNorm层（批量规范化层）

BatchNorm 是一种线性变换（加权平移），加在非线性激活函数前能更有效地调节输出的分布，使激活函数更“活跃”。在卷积网络中，`BatchNorm2d` 对每个输出通道的所有空间位置（跨 batch 和空间维度）进行统一的归一化，然后再用通道独立的缩放和平移参数恢复表达能力。

批量规范化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先规范化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。 正是由于这个基于*批量*统计的*标准化*，才有了*批量规范化*的名称。

请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。 这是因为在减去均值之后，每个隐藏单元将为0。 所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。 请注意，在应用批量规范化时，批量大小的选择可能比没有批量规范化时更重要。

特点：

* 具有可学习参数 $ \ gama 和 \ beita$​
* 作用在：
  * 全连接层和卷积层输出上，激活函数前
  * 全连接层和卷积层输入上
* 对于全连接层，作用在特征维
* 对于卷积层，作用在通道维

---



==总结：==

- `mean` 和 `var` 是**当前小批量**的统计值，仅用于训练时的归一化。
- `moving_mean` 和 `moving_var` 是**长期统计值**，通过指数移动平均累积，专门用于推理模式，确保归一化不依赖于批量大小。
- **训练模式**：使用 `mean` 和 `var` 归一化，并更新 `moving_mean` 和 `moving_var`。
- **推理模式**：直接使用 `moving_mean` 和 `moving_var` 进行归一化。



---

**使用 `Y = gamma * X_hat + beta` 进行缩放和平移**

归一化后的输出计算公式为：

```python
Y = gamma * X_hat + beta
```

其中，`X_hat` 是归一化后的张量（均值接近0，方差接近1）。



**设计初衷**

- **归一化的局限性**：
  - 批量归一化将输入标准化为均值0、方差1（即 `X_hat = (X - mean) / torch.sqrt(var + eps)`）。虽然这有助于稳定训练（通过减少内部协变量偏移），但它限制了网络的表达能力。
  - 如果直接输出 `X_hat`，网络无法学习到适合任务的分布（例如，可能需要非零均值或非单位方差）。
- **引入 `gamma` 和 `beta`**：
  - `gamma`：缩放参数，允许网络调整归一化后输出的方差。
  - `beta`：偏移参数，允许网络调整归一化后输出的均值。
  - 通过公式 `Y = gamma * X_hat + beta`，网络可以学习任意均值和方差的输出分布，恢复或增强模型的表达能力。
- **为何需要缩放和平移**：
  - **灵活性**：`gamma` 和 `beta` 是可学习的参数（通常通过 `nn.Parameter` 定义），通过梯度下降优化，使网络能够根据任务需要调整输出分布。
  - **保持归一化的优点**：归一化（`X_hat`）确保输入到后续层的分布稳定，减少训练中的梯度爆炸或消失问题。
  - **兼容性**：如果网络认为归一化不必要（例如，`gamma=1`, `beta=0`），可以学到接近原始输入的分布，相当于“禁用”归一化的效果。
- **实际意义**：
  - 在卷积层中，每个通道有自己的 `gamma` 和 `beta`，允许不同特征图学习不同的缩放和偏移。
  - 这种设计使批量归一化既能标准化输入（提高训练稳定性），又能保留网络的表达能力（通过学习适当的分布）。





## 11、缓解模型过拟合

### 1、基本概念

训练集：直接参与训练的数据集

验证集：验证训练结果好坏，以便进行后续调整的数据集

测试集：只用一次，用于评估模型好坏

### 2、过拟合和欠拟合

![image-20250813184850091](C:\Users\osquer\Desktop\typora图片\image-20250813184850091.png)

训练误差是模型在训练数据上的预测误差。如果模型在训练过程中无法显著减少这一误差，说明模型可能缺乏足够的复杂性或容量（capacity），无法很好地拟合训练数据中的模式（patterns）。这被称为表达能力不足。泛化误差是训练误差与验证误差的差值，表示模型从训练数据到未见过数据的适应能力。如果泛化误差很小，说明训练误差和验证误差接近，模型对训练数据的拟合程度与对验证数据的表现差异不大。

泛化误差小通常表明模型没有严重过拟合（overfitting），因此可以通过增加模型复杂度（例如增加层数或参数）来进一步降低训练误差，而不会显著损害泛化性能。

泛化误差小的含义: 泛化误差小表明模型没有严重记住训练数据的噪声或特殊模式，而是较为“诚实”地反映了数据的总体趋势。这通常发生在模型过于简单时，限制了其过拟合能力。

“泛化误差很小”: 您可能疑惑为何不直接优化验证误差。泛化误差小只是说明模型未过拟合，但高误差表明模型未达到潜力。

### 3、解决手段

#### 1、欠拟合

可以增加模型层数。

#### 2、过拟合（以下为具体手段）

权重衰减（Weight Decay）和丢弃法（Dropout）是深度学习中常用的正则化技术，旨在防止过拟合（overfitting）并提高模型的泛化能力。以下是对两者应用场景及特点的详细总结，保持专业、清晰的语言结构。

#### **1. 权重衰减 (Weight Decay)**

- 权重衰减通过在损失函数中添加 L2 正则化项，惩罚较大的权重值。其形式为在原始损失 \( L \) 上增加 $\frac{\lambda}{2} \| \theta \|_2^2$，其中 $\theta$是模型参数，$\lambda$ 是正则化强度（超参数）。
- 在 PyTorch 中，通常通过优化器（如 `torch.optim.SGD`）的 `weight_decay` 参数实现。

##### b. 应用场景

- 适用于深层神经网络，尤其是全连接层较多的模型（如 MLP、Transformer）。
- 常用于数据量有限或特征维度较高的情况，以防止模型过于依赖特定权重。

##### c. 实现

- **代码示例**:

  ```python
  import torch
  import torch.nn as nn
  import torch.optim as optim
  
  model = nn.Linear(10, 2)
  optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)  # weight_decay 启用 L2 正则化
  ```

- 优化器在每次更新时自动将权重衰减项加入梯度。

##### d. 特点

- **机制**: 通过缩小权重范数，限制模型复杂度，减少过拟合风险。
- **优点**:
  - 简单高效，易于与梯度下降结合。
  - 对平滑解（robust solution）有正向影响。
- **缺点**:
  - 对所有参数施加相同惩罚，可能不适合需要大权重的层（如批量归一化）。
  - 超参数 $\lambda$ 需要调优。

#### 2、dropout 丢弃法

[参考该连接 ](#2、Dropout层 )

## 12、图像增广

应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。 例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。 我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。优先使用`torchvision.transforms.v2` 

多种图像变换方法仅针对训练集，防止过拟合；测试集不做变换。



### 1. **`ColorJitter` 参数概览**

`ColorJitter` 的构造函数如下：

```python
torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0)
```

- **`brightness`（亮度）**：
  - 类型：浮点数或元组 `(min, max)`。
  - 作用：随机调整图像的亮度。亮度因子从 `[max(0, 1 - brightness), 1 + brightness]` 中随机采样，图像像素值乘以该因子。
  - **常用值**：`0.1` 到 `0.5`，过大的值可能导致图像过亮或过暗，影响模型训练。
- **`contrast`（对比度）**：
  - 类型：浮点数或元组 `(min, max)`。
  - 作用：随机调整图像的对比度。对比度因子从 `[max(0, 1 - contrast), 1 + contrast]` 中随机采样，图像像素值根据公式 `(pixel - mean) * factor + mean` 调整。
  - **常用值**：`0.1` 到 `0.5`，过大的值可能导致图像细节丢失。
- **`saturation`（饱和度）**：
  - 类型：浮点数或元组 `(min, max)`。
  - 作用：随机调整图像的饱和度。饱和度因子从 `[max(0, 1 - saturation), 1 + saturation]` 中随机采样，影响图像的颜色强度。
  - **常用值**：`0.1` 到 `0.5`，过大的值可能使图像颜色过于鲜艳或褪色。
- **`hue`（色调）**：
  - 类型：浮点数或元组 `(min, max)`。
  - 作用：随机调整图像的色调，色调因子从 `[-hue, hue]` 或指定的 `(min, max)` 范围内采样，调整图像的颜色偏向（如偏红、偏蓝）
  - **常用值**：`0.05` 到 `0.2`，过大的色调变化可能导致图像颜色失真。

#### **使用场景及常用参数**

- **`ColorJitter` 常用参数**：

- - 通用任务：`brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1`
  - 轻量增强：`brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05`
  - 强增强：`brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2`
  - 特定属性：仅设置需要的参数（如 `brightness=0.5, contrast=0.5`）。
- **建议**：
  - 使用 `ToImage` 或 `tv_tensors.Image` 显式标记图像数据，避免 transforms 误判。
  - 根据任务需求调整参数，验证增强效果。
  - 结合其他变换（如 `RandomResizedCrop`、`Normalize`）构建数据增强流水线。

### 2、其他变换方法

[参考API文档](https://docs.pytorch.ac.cn/vision/stable/transforms.html#geometry)

## 13、迁移学习

*迁移学习*（transfer learning）将从*源数据集*学到的知识迁移到*目标数据集*。 ImageNet数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。 

```
pretrained_net = torchvision.models.resnet18(pretrained=True)
```

在使用预训练模型之前，必须对图像进行预处理（使用正确的分辨率/插值调整大小、应用推理转换、重新缩放值等）。没有标准的方法可以做到这一点，因为它取决于给定模型的训练方式。它可能因型号系列、变体甚至重量版本而异。使用正确的预处理方法至关重要，否则可能会导致准确性下降或输出不正确。

每个预训练模型的推理转换的所有必要信息都在其权重文档中提供。为了简化推理，TorchVision 将必要的预处理转换捆绑到每个模型权重中。这些可以通过 `weight.transforms` 属性访问

```
import torchvision.models as models
import torch.nn as nn

# 加载预训练 ResNet-50
weights = ResNet50_Weights.DEFAULT
model = resnet50(weights=weights)

# 替换全连接层
num_classes = 10  # 新任务的类别数
model.fc = nn.Linear(model.fc.in_features, num_classes)

# 可选：冻结卷积层以加速训练
for param in model.parameters():
    param.requires_grad = False
model.fc.requires_grad = True

# 定义预处理（保持 ImageNet 归一化）
preprocess = weights.transforms()


```

---



## 14、图像检测技术

### 1、锚框

* 生成锚框 $(s_1, r_1), (s_1, r_2), \ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \ldots, (s_n, r_1).$​,这种组合可以有效减少锚框数量

* 计算交并比，用于衡量锚框和边界框之间的相似性 $J(\mathcal{A},\mathcal{B}) = \frac{\left|\mathcal{A} \cap \mathcal{B}\right|}{\left| \mathcal{A} \cup \mathcal{B}\right|}.$​ 

* 将真实边界框分配给锚框(此时分配的只是标签索引)

  在训练集中，我们将每个锚框视为一个训练样本。 为了训练目标检测模型，我们需要每个锚框的*类别*（class）和*偏移量*（offset）标签，其中前者是与锚框相关的对象的类别，后者是真实边界框相对于锚框的偏移量。 在预测时，我们为每个图像生成多个锚框，预测所有锚框的类别和偏移量，根据预测的偏移量调整它们的位置以获得预测的边界框，最后只输出符合特定条件的预测边界框。

  目标检测训练集带有*真实边界框*的位置及其包围物体类别的标签。 要标记任何生成的锚框。

* 有了带有边界框信息的锚框之后，计算类别和偏移量 $\left( \frac{ \frac{x_b - x_a}{w_a} - \mu_x }{\sigma_x},
  \frac{ \frac{y_b - y_a}{h_a} - \mu_y }{\sigma_y},
  \frac{ \log \frac{w_b}{w_a} - \mu_w }{\sigma_w},
  \frac{ \log \frac{h_b}{h_a} - \mu_h }{\sigma_h}\right),$​  如果一个锚框没有被分配真实边界框，我们只需将锚框的类别标记为*背景*（background）。 背景类别的锚框通常被称为*负类*锚框，其余的被称为*正类*锚框。

* 为锚框分配真实标签

* 使用非极大值抑制预测边界框，因为可能出现多个锚框很相似的情况，此时，可以过滤掉重叠的边界框预测，减少冗余检测，保证效率

==多尺度锚框==

既然每张特征图上都有个不同的空间位置，那么相同空间位置可以看作含有个单元。感受野的定义，特征图在相同空间位置的个单元在输入图像上的感受野相同： 它们表征了同一感受野内的输入图像信息。 因此，我们可以将特征图在同一空间位置的个单元变换为使用此空间位置生成的个锚框类别和偏移量。 本质上，我们用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。

### 2、语义分割

像素标注和预测区域是像素级别的，并且在处理图像时，不再采用直接缩放的方式，因为这样很可能会导致图像像素和标签不匹配，即使它们一一对应，但原图像也发生了改变，训练也就没意义了。

*图像分割*将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以 [图13.9.1](https://zh.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html#fig-segmentation)中的图像作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。

- *实例分割*也叫*同时检测并分割*（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。

### 3、转置卷积

可以增加上采样中间特征图的空间维度，用于逆转下采样导致的空间尺寸减小。

![image-20250819110830115](C:\Users\osquer\Desktop\typora图片\image-20250819110830115.png)

* 在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。 例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。
* 步幅被指定为中间结果（输出），而不是输入。 

### 4、全卷积神经网络

*全卷积网络*（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换，但又与卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。

### 5、样式迁移

 首先，我们初始化合成图像，例如将其初始化为内容图像。 该合成图像是风格迁移过程中唯一需要更新的变量，即风格迁移所需迭代的模型参数。 然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。 这个深度卷积神经网络凭借多个层逐级抽取图像的特征，我们可以选择其中某些层的输出作为内容特征或风格特征。

![../_images/neural-style.svg](https://zh.d2l.ai/_images/neural-style.svg)

 风格迁移常用的损失函数由3部分组成：

1. *内容损失*使合成图像与内容图像在内容特征上接近；
2. *风格损失*使合成图像与风格图像在风格特征上接近；
3. *全变分损失*则有助于减少合成图像中的噪点。