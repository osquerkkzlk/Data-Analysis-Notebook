# 新国大苏州研究院2025 AI 竞赛备赛



## 1.知识补充

### 1.loc和iloc

首先，二者都是pandas中查询数据的方法。不同的是，loc是按名字访问，`.loc[行索引, 列索引]` ，

iloc是按位置访问， `.iloc[行位置, 列位置]`

| 写法    | 索引依据             | 行为特点     |
| ------- | -------------------- | ------------ |
| `.loc`  | **标签（label）**    | 包含结束点   |
| `.iloc` | **位置（position）** | 不包含结束点 |

### 2.append和extend

append会将元素直接添加到列表末尾，很容易嵌套，extend会将可迭代对象中的元素逐个添加到列表末尾。



### 3.按键技巧

1. 切换到下一个标签页	Ctrl + Tab 或 Ctrl + PgDn	
2. 切换到上一个标签页	Ctrl + Shift + Tab 或 Ctrl + PgUp	
3. 快速切换 **桌面**       `Ctrl + Win + ←/→` 



### 4. vlaue_counts

对于pandas数据而言，使用`data.value_counts()`可以统计data数据的类别，最好对series数据处理，比如`data.Sex.value_counts()`



### 5.hist

pandas数据可直接展示数据的数值类型数据的分布 `_ =train.hist(figsize=(20,14))`,同时返回展示的特征名字。可以进行特定条件约束，

`_ =train[train["Survived"]==1].hist(figsize=(20,14))`



### 6.  pd.crosstab

```python
pd.crosstab(
    index=adult_census["education"], 
    columns=adult_census["education-num"]
)
```

作用是**生成一个交叉表（频率统计表）**，行是 `education`（学历类别），列是 `education-num`（学历对应的数值编码），交叉点是**出现次数**。

```python
pd.crosstab(
    index=adult_census["education"], 
    columns=adult_census["education-num"], 
    normalize="index"
)
```

这样会百分比显示

```python
pd.crosstab(
    index=adult_census["education"], 
    columns=adult_census["education-num"], 
    margins=True
)
```

这样会增加总计



### 7.seaborn 快速可视化

```python
_ = sns.pairplot(
    data=data,
    vars=columns, 
    hue=target_column,
    plot_kws={"alpha": 0.2},
    height=3,
    diag_kind="hist",
    diag_kws={"bins": 30},
)
```

| 参数                      | 含义                                      |
| ------------------------- | ----------------------------------------- |
| `data=data`               | 数据                                      |
| `vars=columns`            | 只绘制指定特征列                          |
| `hue=target_column`       | 按目标列（比如是否收入 >50K）进行颜色分类 |
| `plot_kws={"alpha": 0.2}` | 散点透明度为 0.2，方便重叠数据的可视化    |
| `height=3`                | 每张子图的高度为 3 英寸                   |
| `diag_kind="hist"`        | 对角线上绘制直方图，而不是核密度估计      |
| `diag_kws={"bins": 30}`   | 对角线直方图设置 30 个柱子                |

==手写决策规则==

```python
_ = sns.scatterplot(
    x="age",
    y="hours-per-week",
    data=adult_census[:n_samples_to_plot],
    hue=target_column,
    alpha=0.5,
)
```



### 8.drop

```python
data = adult_census.drop(columns=[target_name])
```

删除特定的列



### 9. fit，score，transformer方法

具有`fit`方法的对象称为估计器，此外，它还具有predict（）和transform()方法。

```python
accuracy = model.score(data_test, target_test)
```


 **计算模型在测试集上的准确率（accuracy） ** **执行逻辑**

- `model`：这里指的是一个已经训练好的模型，比如 `KNeighborsClassifier` 或 `LogisticRegression`。
- `.score(X, y)`：
  - 对于分类模型：返回 **准确率**（accuracy） = 预测正确的样本数 ÷ 总样本数
  - 对于回归模型：返回 **决定系数 R²** 分数。
  - 不管是二分类还是多分类，`score` 方法默认都是用 accuracy。

### 10.data.dtypes

`data.dtypes`可以检查数据集中每列的数据类型。



### 11. unique

```
data.dtypes.unique()
```

unique可以找到唯一的数据或类型



### 12.OrdinalEncoder 和 OneHotEncoder 

------

#### 1️⃣ OrdinalEncoder 的本质

- **作用**：把类别变量转换为整数编码，例如：

  ```
  "Low" → 0
  "Medium" → 1
  "High" → 2
  ```

- **重要特点**：整数编码隐含了 **顺序关系**（0 < 1 < 2），也就是模型会“认为”类别之间有大小关系。

------

#### 2️⃣ 对不同模型的影响

- **线性模型（如 LogisticRegression、线性回归）**
  - 线性模型会认为数字之间有连续性和顺序。
  - 如果原始类别本身没有顺序，但你用 OrdinalEncoder，模型可能会误解关系，导致预测性能下降。
  - ⚠️ 例如，把颜色 ["Red", "Green", "Blue"] 编码为 [0,1,2]，模型可能认为 Red < Green < Blue，这是错误的。
- **基于树的模型（如 RandomForest、LightGBM、XGBoost）**
  - 树模型关注的是 **分裂条件**，不会把数字当作连续变量处理。
  - 即使编码不按实际顺序，也不会影响模型性能（树模型只关心某个节点的分裂是否能增加信息增益）。

------

#### 3️⃣ OneHotEncoder 的局限

- **OneHotEncoder** 会把每个类别都变成一列：

  ```
  "Red", "Green", "Blue" → [1,0,0], [0,1,0], [0,0,1]
  ```

- 如果类别非常多（高基数），会导致：

  - 数据矩阵维度急剧增加 → 计算效率降低
  - 可能在树模型中训练更慢

- **总结**：高基数分类变量，如果模型是树，最好用 OrdinalEncoder；如果模型是线性，最好确保编码的顺序合理。

------

💡 **核心思想**：

1. OrdinalEncoder → 带顺序假设，适合有序类别或树模型
2. OneHotEncoder → 不带顺序假设，适合线性模型或低基数分类
3. 滥用 OrdinalEncoder 在线性模型中可能引入“假顺序”错误
4. 单热编码

- 
   这是单热编码（OneHotEncoder）的核心：每个原始类别被拆分成一个独立的列，值为 0 或 1。
-  单热编码通常会先把类别映射成整数（0,1,2…），再进行独热编码，这部分也算是“数值表示形式”的转换。
-  单热编码本身不改变原本是数值的列，它是对类别型特征进行扩展。



### 13、warning

```python
import warnings
warnings.filterwarnings('ignore')
```

忽略警告



### 14、sample

```
data.sample(3) 
```

随机抽取三行



### 15、info()

```
data.info()
```

查看数据的概要信息，比如行数、列数、缺失值分布。这和describle是不同的，后者是显示数值分布情况



### 16、快速查看缺失值

```python
def display_missing(df):        
	for col in df.columns.tolist():                 
    print(f'{col} column missing values:{df[col].isnull().sum()}'
```

或者说，直接 `data.isull().sum()`

### 17、ravel（）

`data.ravel（）` 可以把数组展平成一维数组，便于逐元素计算。



### 18、画图

plt.plot用于绘制折线图，plt.scatter用于绘制散点图

```python
# 水平柱状图

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")


plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)
```



### 19、predict_proba

model.predict_proba(data),可以预测数据的分类概率



### 20、解决非线性问题

1. 使用多项式回归`PolynomialFeatures`
2. （非线性核），即特征映射到高维空间，在高维空间线性可分。
3. 使用可以直接处理非线性问题的模型



### 21、np.log1p 和 np.expm1

二者互为反函数.log1p实际上为`ln(1+x)`，可以对0使用，并且对小数值的精确度更高。



### 22、激活与丢弃

先激活后丢弃。激活是带来非线性变化，丢弃实际上是训练时屏蔽激活值，测试时恢复。



### 23、apply

pandas数据可以使用apply，对每一列或者每一行数据做处理，默认是axis=0，对列处理。



### 24、mse和rmse

- **MSE（Mean Squared Error）**：均方误差

$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

- **RMSE（Root Mean Squared Error）**：均方根误差,并且单位和y一样，方便解释。

$RMSE= \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$



### 25、 过拟合与欠拟合

当数据量少，并且存在噪声时，就会发生过拟合，此时模型不知学习到了数据特点。也学到了噪声。欠拟合受限于模型结构等原因，学习能力差，虽然数据量大，噪声比较小，但学习的特征并不全面。我们就是要在过拟合和欠拟合中找到一种平衡。

训练误差和测试误差差距很大时，通常会过拟合。我们想找到最佳点（`Sweet Spot`）。

换一种角度，当数据量很小时，模型关注的特征也很少，因此会过度学习，发生过拟合。

数据集中存在不可约噪声，即模型无法学习的特征，因为噪声本身就是随机的。



### 26、ShuffleSplit

随机交叉验证器



### 27、评分指标

这是 **scikit-learn 的设计约定**，原因是 **scikit-learn 所有评分指标（scoring）都遵循“分数越大越好”的统一规则**。但是像 **MAE、MSE、RMSE 这类误差指标** 是 **“越小越好”** 的，所以如果直接用正值，模型评分会和框架的逻辑冲突。

```python
scoring="neg_mean_absolute_error"
```

scikit-learn 实际做的是：

- 先计算 **`mean_absolute_error`**（正的误差值）
- 再取 **负号**（乘以 `-1`），返回一个“负的分数”。

这样一来，误差越小，负数的绝对值越小，但由于更接近 0（或更大），就能满足“**分数越高越好**”的框架统一规则。

| 指标 | `scoring` 名称                | 取值含义                        |
| ---- | ----------------------------- | ------------------------------- |
| MAE  | `neg_mean_absolute_error`     | 返回负的 MAE，越接近 0 说明越好 |
| MSE  | `neg_mean_squared_error`      | 返回负的 MSE                    |
| RMSE | `neg_root_mean_squared_error` | 返回负的 RMSE                   |



### 28、显示某一列数据的分布

```
data.plot.hist(bins=10, edgecolor="black")

```

将数据划分为 10 个等宽区间，`edgecolor` 是边框颜色。



### 29、误差指标

MAE 绝对误差

MSE 、**RMSE（和数据单位一致，解释性强）** 相对误差



### 30、cross_val_score

可以快速评估交叉验证分数，这一点和cross_validate很相似，只不过只取了评估分数。参数`scoring`可以选择多个评估指标，其中`balanced_accuracy`分别计算不同类的准确值，再平均，可以有效处理类别不平衡的问题、`accuracy`直接计算准确值，很可能会忽略少数类而偏向于多数类。



### 31、使用  KNeighborsClassifier

使用之前，先对数据进行标准化，保证各个特征的贡献度相同。



### 32、train_test_split

需要注意的是，结果的顺序

```python
x_train,x_val,y_train,y_val=train_test_split(train_data,target_data,random_state=42,shuffle=True)

```



### 33、 item  和  detach

`item `把只含有一个标量的张量转换成python的数值类型（float或者int），他要求必须是单值张量才能使用该方法，同时会剥去梯度信息，复制了一个新的值出来。

detach是把张量从计算图中分离出来，得到一个新张量，没有梯度信息，但与之前的张量共享同一片内存信息。当你想对新数据就地更改时，为了不影响原有的数据，使用如下方法`data.detach().clone()`



### 34、np.arange

`np.arange()` 不会包含终点，作用类似于range



### 35、BCEWithLogitsLoss （二分类任务）

工作原理是：函数内部先进行sigmoid，转换成概率，原先的值越大，概率就越大；数值越小，概率就越小。（之前为正数，表示预测1的概率更大；之前为负数，表示预测0的概率更大。总之绝对值越大，模型越自信）。

![image-20250831164845577](C:\Users\osquer\Desktop\typora图片\image-20250831164845577.png)

之后再经过 BCEloss ，$BCE(y,y^、)=−[ylogy^、+(1−y)log(1−y^、)]$ ，其中y` 为转换成的概率，结果为损失（损失越小，越好）。



### 36、loss注意点

```
loss(pred,y_true)
```

pytorch需要预测值在前，损失值在后。sklearn则需要真实值在前，预测值在后。



### 37、make_column_selector 

```python
from sklearn.compose import make_column_transformer
from sklearn.compose import make_column_selector as selector

preprocessor = make_column_transformer(
    (categorical_preprocessor, selector(dtype_include=object)),
    remainder="passthrough", # 保留原始特征
)
```



### 38、树模型特点

1. 不需要缩放，因为树模型看顺序和阈值
2. 类别可以采用 OrdinalEncoder 编码，可以减少特征维度，防止梯度爆炸。



### 39、确定模型最佳超参数

1. 对非数值列处理，进行编码，可以`OrdinalEncoder` ， 也可以 `OneHoteEncoder` 
2. 建立模型
3. 设置搜索的参数，并进行搜索
4. 找到最佳参数，也可以选择显示所有参数



### 40、rename（重命名）

```python
cv_results = cv_results.rename(shorten_param, axis=1)
```

shorten_param为重命名方法



### 41、rsplit（从右边开始分割字符串）

```python
str.rsplit(sep=None, maxsplit=-1)

```

表示从右边开始分割字符串，sep为分隔符，maxsplit表示分割次数



### 42、嵌套交叉验证

先进行CV ，把数据拆分成外层训练集和测试集；后进行内层CV，在外层训练集上做超参数搜索和交叉验证（此时内部自动划分训练集和验证集）；之后用外层训练集做二次拟合（最终训练），得到一个模型，并对这个模型进行交叉验证评估。

<img src="https://inria.github.io/scikit-learn-mooc/_images/nested_cross_validation_diagram.png" alt="Nested cross-validationdiagram" style="zoom:50%;" />

### 43、RandomizedSearchCV

**`n_iter`** 代表了迭代次数，也就是说他会从所有参数组合中随机选择该次。



### 44、np.logspace

该函数会生成等比数列，生成指数范围内的参数值。start为起始值，stop为结束值，num为生成的样本数量，base为底数。

```python
np.logspace(start, stop, num=50, base=10.0)

```



### 45、astype （强制转换类别）

```python
data.astype("float")
data.astype(np.float)
```

astype强制转换时，会向零取整，删除小数部分，并且更推荐第二种写法，似乎第一种np内部会改变类型（尽管二者起到的作用都是相同的）？



### 46、cv

cv是交叉验证的方式，sklearn内部会自动对数据和对应的标签做相同的划分。



### 47、pandas 特性

1. 对 pandas 数据赋予一个之前不存在的列名，pandas就会自动创建该列。

2. pandas数据的columns属性会返回列名，但是它本身仍然是pandas数据（series），这点非常重要。



### 48、balanced_accuracy

当不同类别的数目不平衡时，推荐这个scoring参数。



### 49、sort_values()

对于pandas数据而言，可以以某一列为依据，对整个数据进行重新排序。ascending表示升序。

```python
cv_results.sort_values(by="test_score",ascending=False)

```



### 50、list(x:str)

字符串强制转换为列表时，字符串的每一个字符都会被拆分开，视为单独的单元。



### 51、reset_index

该函数会重新排列数据的索引，通过 drop参数可以选择把之前的索引丢弃，inplace参数可以选择就地更改。

```python
cv_results.reset_index(drop=True,inplace=True)
```



### 52、硬预测和软预测

predict：硬预测（直接预测类或者值）

predict_proba : 软预测（预测每个类的概率）



### 53、==线性模型的参数==

coef_  代表斜率，是一个==一维数组==

intercept_代表截距，是一个==数字==



### 54、sklearn数据特征

sklearn数据应该是 `(n_samples,n_features)` 的二维特征。



### 55、np.concatenate

np.concatenate 用于拼接数据，而pd.concat也用于拼接数据，不要搞混了。



### 56、Nystroem 

`Nystroem` 可以把原始特征映射到高维空间，并且可以自己设定高维空间的维度` n_components` ，从而可以控制特征数量。大体过程是选取一些数据作为基点，然后计算其他数据与这些基点之间的相似度，归一化之后，计算特征矩阵。但是这些特征的解释性不如 `PolynomialFeatures` 解释性强。

默认的 kernel 是 “rbf”（高斯径向基）核，它可以把数据映射到无限空间，让模型也可以解决非线性问题。其中，还有一个参数gamma，其控制着核函数的”平滑度“，当gamma大时，两个点稍微远一点就会被视为不相关，当gamma小时，即使两个点相距很远也会被视为相关。（核函数结果趋向于1表示相关，趋向于0表示无关）。



### 57、ValidationCurveDisplay

有一些知识点：

1. 参数scoring表明评估指标
2. 参数score_name 表明图例上的名字
3. negate_score表明绘图或显示的时候取负号。
4. 方法 `from_estimator()` 可以直接把模型作为参数，省去了`validation_curve()函数`的步骤。



### 58、ListedColormap（自定义颜色映射）

```python
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colors import ListedColormap

X = np.random.rand(10,2)
y = np.array([0,1,0,1,0,1,0,1,0,1])

cmap = ListedColormap(["tab:red", "tab:blue"])
plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap, s=100)
plt.show()

```



### 59、子图

plt.subplot()分别创建子图，每次调用都会切换一个子图，返回当前子图对象。

plt.subplots()一次性创建子图，返回画布和所有子图对象。



### 60、逻辑回归

该模型是线性模型，直接学习数据分布，可能学不到真实的规律，这时候可以先对数据处理，使其离散化，之后再学习就好多了。



### 61、KBinsDiscretizer

箱线图，只是一种特征变化，无法捕捉到特征之间的交互关系，（SplineTransformer也是如此）



### 62、==transformer （经典机器学习）初谈==

通过利用非线性特征工程，可以使用逻辑回归等线性模型对数据集分类。`KBinsDiscretizer` 和 `SplineTransformer` 等 Transformer 可用于为每个原始特征独立设计非线性特征。这些转换器只能对某些特征作编码处理将连续值转换成易于学习的离散值，但是却无法捕获原始特征之间的相互作用。	

`PolynomialFeatures` 和 `Nystroem 等 Transformer` 可用于设计捕获原始特征之间相互作用的非线性特征。因此，我们往往可以把二者结合起来，对数值要素应用分箱或样条变换以及对分类特征应用单热编码可能会很有用。然后，可以将生成的特征与核近似（大大减小计算量）相结合，以模拟数值和分类特征之间的交互作用。这可以在 `ColumnTransformer` 的帮助下实现。



### 63、线性可分离

存在一条直线或者一个平面可以把两个类完全分开。



### 64、线性模型

线性回归优先使用岭回归，**alpha**代表正则化强度，alpha越大表示正则化力度越大。线性模型倾向于给小数值特征分配大权重，大数值特征分配小权重。而岭回归则会正则化大权重，减小其权重，出现“过度惩罚”。



### 65、数据缩放

`MinMaxScaler`会将数据缩放到指定范围，默认是1，通常并不会改变数据的原有分布情况，只是会缩小数值范围，对异常值非常敏感、`Standscaler`会将数据进行标准化，均值为0，方差为1，数据大多集中在-3与3之间。

1. 技巧：`MinMaxScaler`后跟`PolynomialFeatures` 好处是会把数据缩放01之间，即使特征交互，数据范围仍在01之间，后面跟上`Radge`的话，对每个特征更公平。而标准化的缺点是，特征交互时，可能会出现不同特征范围不同，相当于没有标准化。



### 66、RidgeCV

内置交叉验证的岭回归，会自动找到给定范围内的最优alpha，其中store_cv_results为True时，会把每个alpha对应的交叉验证值存储在cv_results_中。



### 67、aggregate(["mean", "std"])

```python
cv_alphas = cv_alphas.aggregate(["mean", "std"])
```

aggregate表示对数据应用这两个函数，沿着行增大的方向。



### 68、逻辑回归中的惩罚项 C

对于逻辑回归而言，C越小意味着惩罚越严重，这就使得为了保证损失小，所以模型的权重就会较小，而z就会跟着小，之后经过sigmoid之后，概率在0.5左右，模型置信度低，所以模型不敢确定类别。
$$
z=wTx+b
$$

$$
\text{Loss}(w) = \frac{1}{n} \sum_{i=1}^{n} \log\Big(1 + e^{-y_i (w^T x_i + b)}\Big) + \frac{1}{2C} \|w\|_2^2
$$

并且，如果原本的特征就足以进行线性分离，效果也不错。那就没有必要增加特征维度（复杂性），因为随着特征维度的增加，模型很容易过拟合。



### 69、箱线图

data.box_plot()绘制箱线图，参数vert=True，用于调节横纵坐标显示次序。

plt.tight_layout()用于自动调节标签位置等，可以默认加上



### 70、OnehotEncoder

当我们划分测试集和训练集的时候，有时候一些特征中的类别只在某个数据集上出现，这时候就会报错，所以我们最好设置参数 handle_unknown="ignore"



### 71、变换器

通常变换器会生成  numpy  数组。



### 72、RandomResizedCrop（裁剪图像）

```python
transforms.RandomResizedCrop(32,scale=(0.62,1.0),
                            ratio=(1.0,1.0))
```

参数最好是浮点数。需要注意的是，32是变换之后的图像的长和宽，即使经过scale之后的图像面积不符要求，图像也会被缩放到32的，这点放心。



### 73、张量拼接

张量拼接有两种策略，一种是torch.cat，这种方式，不会增加维度，沿拼接维度长度增加。另一种是torch.stack，这种方式会增加维度，新增的维度就是拼接维。可解决形状如`list[tensor]`的数据。二者在拼接一维数据或者二维数据（但实际上是一维数据）时体现不出差异，但是拼接多维数据时就体现出来了。直白点讲，cat是你想的那种方式，一行排列一行，stack则是一坨堆在一坨上😂。

但要记住，numpy中是concatenate。



### 74、损失（torch）

默认参数reduction="mean"，也就是说计算这一批数据的损失平均值。



### 75、argmax

`torch.argmax(data)`默认会把数据展平后找到最大值



### 76、sum

张量数据求和，直接`data.sum()`即可，这样返回的仍然是一个张量，并且计算效率高；相反，如果采用`sum(data)`,该方式是利用py内置的sum函数。它会把数据视为可迭代对象来处理，效率低。

​	

### 77、item

当我们想在张量计算后得到一个数值时，使用`data.item()`的方法，因为张量计算的结果仍然是张量，即使他是单值张量，所以我们就需要把他转换成数值。



### 78、元组

```python
self.x=x,
```

实际上，会把x视为一个元组，因此切记不要再数据后面加上逗号。



### 79、视觉模型

```
Conv → BatchNorm → ReLU
```



### 80、GAP（全局平均池化）

本质上把每个通道看作一个特征，求取该通道里面的平均值。



### 81、卷积

研究发现少量“跨层非线性”不会立即导致退化，反而可能更好训练（这在 ResNet/Bottleneck 结构里比较常见）。**卷积堆叠必须周期性插入非线性，否则网络退化为线性模型**。



### 82、激活函数

```
nn.ReLU(inplace=True)
```

该函数内部存在 inplace 参数，表示张量是否就地更改。就地封盖不会产生新张量，节省内存（特别是在计算资源紧张的设备上）。



### 83、torchvision.io.read_image

该函数读取的结果是张量



### 84、标签（torch）

对于torch来讲，他应该是个long类型



### 85、LabelEncoder

该函数用于单列数据处理，为离散值分配标签（从零开始），返回一个1d ndarray



### 86、array和ndarray



array是广泛的概念，通常指数组和列表结构，np.ndarray()是np的核心数据结构。np.array()实际上调用的是np.ndarray()



### 87、ToPILImage

transforms.ToPILImage可以把张量转换为图像（输入张量要求在（0 ~255）或者（0 ~1））



### 88、listdir（）

该函数可以列举出某文件夹下的文件名，但注意它并不是按名字顺序去读取，因此最好最外层加上sorted函数并设置参数key，确保文件排列顺序正常。



### 89、学习率调度器（精简版）

 `StepLR `会每隔几个步长将学习率乘上一个衰减因子 gamma，通常是0.1或者0.5。适合数据集比较小，模型结构比较简单的场景，需要自己根据绘制的图像设置步长 step_size（最好一开始不加，先训练一下视情况再加）。

```python
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
```

` MultiStepLR` 在指定的 epoch（milestones）处将学习率乘以 gamma。

```
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.1)
```

`ReduceLROnPlateau` 会在指标停止改善时，将学习率乘上 gamma。这种调度器可以根据模型性能动态降低学习率，适合复杂任务

- mode：监控指标的模式（min 表示损失，max 表示准确率）。
- factor：衰减因子（类似 gamma）。
- patience：等待指标未改善的 epoch 数。

```
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)

```

**动态调整**：根据模型性能（如验证损失或准确率）动态降低学习率，适合复杂任务或迁移学习。

- **您的 CIFAR-10 任务**：强烈推荐，因为预训练 ResNet-18 需要低学习率（3e-4），且 CIFAR-10 训练可能遇到损失波动，ReduceLROnPlateau 可根据损失动态调整。
- **不稳定训练**：当损失或准确率在某些 epoch 停滞时，自动降低学习率。

- **注意**：需提供验证损失或准确率，计算开销略高。



### 90、Dataloader特性

Dataloader会返回一个元组，因此我们需要进行解包，常见的操作如下

```
for (batch,) in dataloader:
```



### 91、DecisionBoundaryDisplay (绘制决策边界)

```python
import matplotlib.pyplot as plt
import matplotlib as mpl

from sklearn.inspection import DecisionBoundaryDisplay

tab10_norm = mpl.colors.Normalize(vmin=-0.5, vmax=8.5)
# create a palette to be used in the scatterplot
dbd = DecisionBoundaryDisplay.from_estimator(
    linear_model,
    data_train,
    response_method="predict",
    cmap="tab10",
    norm=tab10_norm,
    alpha=0.5,
)
```

from_estimator方法实际上是从模型建立决策边界。data_train是训练数据，也就是决策边界生成的依据。cmap是颜色映射，“tab10”是一种颜色映射方案。response_method表明函数以哪种方式来绘制决策边界，一般有predict（显示类别，离散），predict_proba(连续，渐变，显示概率)。norm表明不同类别的分配情况，vmin和vmax表明了某类别划分的颜色映射的区间，比如,-0.5~0.5为一种颜色，正好对应了类别0，之后会在cmap里面找到对应对的颜色映射，完成颜色分配。



### 92、绘制离散点（sns）

```python
sns.scatterplot(
    data=penguins,
    x=culmen_columns[0],
    y=culmen_columns[1],
    hue=target_column,
    palette=palette,
)

```

sns的风格是传入pd数据和两个列名（字符串），hue是颜色编码的依据，也就是说函数会根据每个数据的类别不同分配不同的颜色，palette是颜色映射表。

其中，color参数是统一设置所有点的颜色，palette只有在hue参数存在的前提下使用，表示对每个类进行分别映射。

该函数只接受x和y作为数据输入，即使不是pd数据也要写上这两个参数，并且只能是一维数据。

参数linewidth可以控制散点边框线的粗细。

facecolors参数控制填充颜色，当值为“none”时，内部无填充。

参数marker用于指定散点图中点的形状

### 93、图像标签

```python

plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
_ = plt.title("Decision boundary using a logistic regression")
```

bbox_to_anchor表明图例对齐的点在1.05倍的长和宽处（说白了就是坐标位置点），loc表明对齐的位置是图例的左上方点。



### 94、参数模型和非参数模型

参数模型是指模型的学习参数是固定的，非参数模型是指模型不依赖固定数量的参数来拟合模型，会根据寻来你数据量的多少去调整模型复杂度。



### 95、树模型的可视化

你要知道树模型在拟合后，他的划分规则实际上已经确定了，因此我们既可以使用决策边界展示函数，也可以采用更详细的tree_plot函数。其中，参数feature_names表示数据特征（应该与训练数据的特征保持一致），class_names表示叶子节点的名称，一般就是    class_names=tree.classes_.tolist(),因为tree.classes _ 会返回一个ndarray的数据，因此需要把他转换成列表才行。inpurity则表示咋杂质，也就是表明划分的好不好。

```python
from sklearn.tree import plot_tree
_,ax=plt.subplots(figsize=(8,6))
_=plot_tree(
    tree,
    feature_names=culmen_columns,
    class_names=tree.classes_.tolist(),
    impurity=False,
    ax=ax
)
```



### 96、决策树分类器

该分类器在依据某个特征划分之后，会预测分区中表示最多的类。



### 97、新建数据（pd）

新建数据的时候最好使用pd.Dateframe，直接成为规范即可，没有理由。



### 98、pd数据特性

data[[column]]得到dataframe数据（二维），data[column]得到一维数据



### 99、训练与预测（sklearn）

加入训练的啥时候使用的pd数据，那么预测的时候也应该使用pd数据，并且预测特征名字应该和训练特征名字相同。



### 100、绘图颜色

```
'tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan'


```



### 101、决策树的超参数

`max_depth` 会在特定水平上水平切割树，无论树生长是否有利。



### 102、%%time

在jupyter的单元格内运行，可以显示单元格的运行时间。



### 103、bagging

基础模型越接近真实数据生成过程，预测结果就越精确。Bagging主要降低方差，并不降低偏差，体现了Bootstrap的理念（即有放回抽样，这样的话大概有63.2%的数据会被至少选中一次，剩下的数据不会被选中）



### 104、cv_results（搜索日志）

对于搜索超参数的cv方法，会有一个属性叫做 cv_results_，里面记录着本次搜索的超参数以及模型评估指标，运行时间。并且，搜索的超参数，比如 max_depth 在前面加上param _，就变成了param_max_depth。



### 105、bagging与随机森林

bagging和随机森林都会采用bootstrap随机取样，但不同的是bagging只会进行一次特征采样，而随机森林则会多次特征采样（子节点分裂时）。



### 106、编码

OneHotEncoder编码格式处理未知数据时，需要采用 handle_unknown="ignore"的方法。对于OrdinalEncoder编码处理未知格式数据时，需要采用handle_unknown="use_encoded_value", unknown_value=-1



### 107、make_column_transformer

会针对不同的列采样不同的处理方法，但需要注意的是参数remainder，默认是"drop"，也就是会丢弃没有指定处理的列，或者我们可以主动设置参数"passthrough"，这样就直接保留原数据（也就是未作处理的数据）。



### 108、编码格式对比

`OneHotEncoder` 适合类别数较少，适合线性模型或者距离度量模型，因为这类模型对数值大小敏感，如果采用`OrdinalEncoder`的话，可能会学到糟糕的类别大小关系，这是不合理的。此外，先做独热编码，后座特征交叉（`POlyNormialFeatures`），可以构造出有用的特征

`OrdinalEncoder` 会把特征进行整数编码，可以对多列进行处理，适合树模型，因为这类模型能更好的进行分叉（比如大于或者小于某个阈值）。

`LabelEncoder` 其实和`OrdinalEncoder`很像，也是整数编码，不同的是，他只能对一列数据进行编码，但好在它的返回结果是 ndarray 的数据，方便进行后续转换。



### 109、集成模型

RandomForestClassifier 中max_features参数默认采用了“sqrt”策略，也就是说模型会自动把max_features设置为输入数据特征数的根号二倍。而其他模型则会禁止特征子采样，也就是会采用全部特征去回归或者分类。



### 110、nonzero、flatnonzero

二者都是返回非零值的索引，但是又有一些差别。其中 nonzero 是按照原数据形状返回结果，而flatnonzero则是先把原数据展平后，再返回结果。



### 111、np.intersect1d

该函数会返回两个数组的交集



### 112、AdaBoost（boosting）

boosting与bagging不同：在这里，我们从不重新采样我们的数据集，我们只是为原始数据集分配不同的权重。

参数n_estimators，boost是说每次拟合都是同样的训练数据，然后找出每次拟合错误的数据，并给他们分配较高的权重，使得模型更注意他们，重复操作，也就是说n-estimators参数就是这个重复操作的次数。最终拟合的模型有属性estimator_weights _表示最终模型的权重，estimator_errors _ 表示弱学习器在训练集上的加权错误率，模型就是根据该属性为不同弱学习器分配最终权重的。



### 113、GBDT（梯度提升决策树）（boosting）

全名`Gradient Boosting Decision Trees`

即使 AdaBoost 和 GBDT 都是提升算法，它们的性质也不同：前者为特定样本分配权重，而 GBDT 将连续决策树拟合到其前一树的残差（因此称为“梯度”）。因此，集成中的每棵新树都试图通过专门解决前一个学习器所犯的错误来完善其预测，而不是直接预测目标。也就是说上一颗树的残差会被当作标签，用于预测新的给定数据的残差，那么最终预测出来的也就是上一颗树预测的差距，二者相加或者说加权就能逐步逼近原始数据分布.



### 114、ax.set_xlim

```python
ax.set_xlim([x_min,x_max])
```

设置坐标轴从x_min到x_max,不在该范围的曲线将被隐藏。



### 115、bagging和boosting

bagging是强学习器，也就是说子模型需要过拟合，而boosting是弱学习器，子模型通常会欠拟合。

超参数调整技巧，如下：

1. **bagging** 。随机森林有参数 n_estimators 表示子树的数量，数目越多泛化性能越好，但是会减慢拟合时间。参数  max_features  会控制在生长树时寻找最佳拆分时需要考虑的特征大小，较小的值则需要进行更多的子树以达到同样的效果，如果设置为none等同于会考虑所有的特征。max_depth限制了子树的最大深度。参数max_leaf_nodes控制最多有多少个叶子。参数min_samples_leaf控制每个叶子最少有多少个样本，但是太大的值反而会影响性能。

2. **boosting**。直方图梯度提升树，参数 max_depth控制每棵树的最大深度，表示每棵树拟合程度大小，一般较小（毕竟是弱学习器）。learning_rate控制每次矫正的贡献程度。max_iter表明树的数目，也就是迭代的次数，但是他会收到早停的影响，一般较大，并且learning_rate较小，二者相互配合，达到不错的程度。

3. 基本处理策略

   1. **快速 baseline** → `RandomForestRegressor`

      1. 稳定，不容易出错，少调参就能跑。

   2. **中等数据集**（1k~100k 样本） → `GradientBoostingRegressor`

      精度比随机森林高，可调 `n_estimators` + `learning_rate`。

   3. **大数据集** (>100k 样本) → `HistGradientBoostingRegressor`

      快速高效，可使用早停（`early_stopping=True`）。

4. **特别任务**（特征稀疏/异常样本多） → 可以尝试 AdaBoost 或者 XGBoost/LightGBM。

------

4️⃣ 竞赛实用策略

- **先用随机森林**：快速 baseline，看数据特性。
- **再用  GBDT**：提升精度，可调学习率+树深度。
- **可加堆叠**（stacking）：最终融合多个模型，尤其是 RF + GBDT + LightGBM，通常能进一步提升 1-2%。

### 116、KBinsDiscretizer (分箱离散化器)

将连续数据离散化到特定区间（根据值的范围划分区间，并把同一区间的数值视为一类），你要记住它不会忽略本就离散的特征，因此，最好使用 columntransformer。该方法可以加速梯度提升，因为分箱会减少候选分割点，也就是说模型在子节点分裂时为搜索最优划分点的时间就明显少了（事实上HistGradientBoostingClassifier和HistGradientBoostingRegressor已经做了该步骤）。



### 117、negate参数

在验证曲线显示时，比如  ValidationCurveDIsplay  函数，里面有参数negate，当设置为True时，则会自动把评估分数取反，常用于评估指标为损失时。



### 118、HistGradientBoosting（分类器或者回归器）

原生支持缺失值数据，会自动根据潜在收益进行分配，而其他的树则处理不了缺失值。



### 119、预测因子

通常是指数据特征。



### 120、嵌套交叉验证

内层和外层的评估指标可以相同也可以不同，因为内层的scoring用于选取参数时的提供标准，外层的scoring用于交叉验证评估模型的性能。并且你要注意的是交叉验证返回的结果是一个字典，一般你需要把它转换成pd数据。



### 121、快速删除缺失值

```python
dataset = dataset[feature_names + [target_name]].dropna(axis="rows", how="any")
```



### 122、sample（随机抽取数据）

```python
dataset = dataset.sample(frac=1,random_state=0).reset_index(drop=True)
```

sample表示随机抽取数据，frac表示抽取数据占的比例，reset_index会重置索引。



### 133、参数n_estimators 和参数max_iter

前者是随机森林和梯度提升的子树数目，后者是直方图梯度提升的子树数目。



### 134、评估指标

sklearn都是（y_true,y_pred），对于pytroch，内置的评估指标是（y_pred,y_trye），此外也可以自定义评估指标。



### 135、drop方法

对于pandas数据而言存在drop方法，有参数labels可以直接指定要删除的行列名字，axis参数表明删除的是行还是列，columns可以接受多个列名，但必须是列表的形式，index可以接受多个行名，也必须是列表的形式。



### 136、np数据和torch数据

torch.tensor(data)会拷贝数据，不会共享内存，更安全，接受device，dtype等参数。

torch.from_numpy(data)会零拷贝，共享内存，更快,但是只接受np数组这一个参数。



### 137、创建子图

plt.subplot()是分别创建子图，而plt.subplots（）是一次性创建子图。



### 138、len函数

len(data)，该函数只会返回数据最外层维度



### 139、v2.RandomAffine

```python
    v2.RandomAffine(degrees=15,translate=(0.1,0.1),scale=(0.9,1.1),shear=10)
```

degree为随机旋转的角度范围，translate表示在水平方向和垂直方向最多平移 10%，scale表示随机缩放到原始大小的0.9~1.1，shear表示随机错切的范围是-10 ~10.



### 140、基线模型

**DummyClassifier** 

参数strategy="most_frequent"表示模型始终预测数量最多的那个类，当类别不平衡时，可以用。strategy="stratified"，都没个样本按训练集的类被分布随机抽样一个标签。

**DummyRegressor**

参数strategy=“mean”：始终预测训练集的平均值



### 141、交叉验证

1. KFold ，训练集最好选取shuffle=True，因为这样可以避免原始数据的排列对结果的影响。
2. StratifiedKFold，可以保证划分后的数据集（训练集和验证集）的数据类被分布和原数据集一致。
3. 嵌套交叉验证的思想在于：在选择最佳参数时，我们是通过测试集上的测试分数来确定哪个参数是最佳参数的，但是这样的话，模型在当前测试集上的分数高，但可能他发生了过拟合，换一个数据集分数就一般了。所以，我们提前划分数据集，让模型在未见过的数据集上评估性能。



### 142、标签

当标签是字符串类型时，深度学习应该把他们映射成数字。经典机器学习中的回归技术也需要映射成数字，而分类技术则不需要。



### 143、评估指标

1. accuracy_score (Y_true,Y_pred)评估模型准确率，当然也可以直接用model.score(data,target)评估。整体上预测对的比例。
2. precision_score(精度)：被模型预测为正类的样本中，真正是正类的比例。
3. balanced_accuracy_score:各类别召回率的平均值，这所以不是精度的平均，这是因为当类别不平衡时，模型倾向于预测多数类，这样预测成少数类的前提下本来就是少数类的可能性非常大，如果选择精度的话，反而会显得模型性能优秀，但这是错误的’而选择召回率会改善这种情况。

| 指标         | 英文名                        | 公式                                                        | 含义                                 | 关注点 / 适用场景                               |
| ------------ | ----------------------------- | ----------------------------------------------------------- | ------------------------------------ | ----------------------------------------------- |
| **准确率**   | Accuracy                      | $(TP + TN) / (TP + TN + FP + FN)$                           | 总体预测正确的比例                   | 类别平衡时最直观，整体性能指标                  |
| **精度**     | Precision / 查准率            | $TP/(TP+FP)$                                                | 被预测为正类的样本中，真正正类的比例 | 误报成本高的场景（如垃圾邮件、肿瘤筛查）        |
| **召回率**   | Recall / 查全率 / Sensitivity | $TP/(TP+FN)$                                                | 所有真正正类中，被预测为正类的比例   | 漏报代价高时使用（如病人漏诊）                  |
| **F1-score** | F1                            | $2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$ | 精度和召回率的调和平均               | 当 Precision 和 Recall 都重要时（如类别不平衡） |
| **特异性**   | Specificity                   | $TN/(TN+FP)$                                                | 真负样本被正确识别的比例             | 多用于医学诊断，关注负类预测正确性              |



### 144、显示函数（sklearn）

`PrecisionRecallDisplay` 之类的函数会返回一个包含所有绘图元素的显示，显示器有一个ax_属性，你可以像处理ax那样调整图像。



### 145、make_scorer（设置评分）

当scoring参数指定评估标准，并且他还需要传入额外参数，比如pos_label（指定哪个类（字符串类型的）是正类）。

```
score = make_scorer(precision_score, pos_label="donated")

```



### 146、pd.factorize(编码)

该方法用于标签编码，同时  pd.dummies用于独热编码。pd.factorize（data）它会返回两个变量，第一个是编码后的数据，数值形式，第二个是编码的标签（映射方式），字符串形式。



### 147、聚类  

1.   KMeans  方法，无监督学习，拟合数据之后，有一个属性labels_ ， 表示训练数据被分配的标签‘有一个方法predict，用于预测新数据的标签，对于训练数据而言，结果和labels_一致。参数  n_init  控制整个聚类过程进行多少次，最后会取  inertia  最小的那个作为输出。max_iter 参数控制每次聚类会迭代的次数，但是当结果不再放生变化时就会提前停止。方法   fit_predict（）可以拟合并预测每行数据的聚类标签，方法   fit_transform（） 可以拟合并计算每个数据点到各个聚类中心的距离。



### 148、silhouette_score（轮廓系数）

范围在-1和1之间，越接近1表示效果越好，越接近0表示数据被分到了两个簇的边界，越接近-1表示数据接近另一个簇（错误分配）。



### 149、数据标准化

你需要注意的是，数据标准化，只是让每个特征对结果的影响尽可能保持公平。有时，某些特征对结果影响很大，此时进行标准化，反而会拉低分数，所以对标准化要慎重看待。



### 150、isna(pandas数据)

data.isna()会按列返回缺失值，一般和sum（）连用以求出每列缺失值的数量。isna()和isnull()函数完全等价。



### 151、数据可视化

1. plt.hist(data,bins="auto",edgecolor="black")
2. plt.pie(data,labels,autopct="%.1f%%"),单个%会被当作占位符来使用。%%才会显示成%



### 152、kaggle提交

kaggle默认就工作在\kaggle\working目录下



### 153、where和apply

np.where(a,x,y) ，如果条件满足返回x，否则返回y，这是矢量化运算，底层用c实现，速度非常快,返回的是ndarray的格式，如果神经网络处理，最好先取数值，避免嵌套。

data.apply(),限于pd数据，本质是一个for循环，对于大型数据非常慢。



### 154、cbrt

np.cbrt()立方根函数



### 155、pd.qcut()

```
pd.qcut(df['balance'], q=20, labels=False, duplicates='drop')
```

按照数据的分位数进行切分。duplicates='drop'表示自动丢掉重复的箱子。



### 156、plt.legend()

设置图例，    plt.legend(bbox_to_anchor=(1.05,1),loc="upper left")



### 157、roc_auc_score

该分数需要你体哦那个连续的概率值，以便模型选择阈值，因此你不能直接硬预测，而应该

predict_proba（data）[:,1],之所以选择第二列，是因为我们希望分数越大表示模型越能相信是正类。



### 158、transform变换

方法fit_transform实际上是学习传入数据的特征，如均值、方差等，之后再对其进行变换，一般用于训练集。而transform方法根据之前学到的特征进行变换，一般用于验证集和测试集。



### 159、combinations

itertools是python内置的迭代器模块，combiantions会生成无重复的元素组合

```python
combinations([A, B, C], 2)
(A, B), (A, C), (B, C)
```



### 160、dataloader 和dataset

dataset只需要实现一个数据的取值即可，因为dataloader一次只提供给它一个索引，然后逐步把他们包装成batch。



### 162、requires_grad

当你创建一个新的module对象时，其默认参数的权重和偏置都是可训练的。



### 163、分类任务

保持标签为一维即可。



### 164、dropna方法

```python
data.dropna(axis=0,subset=["A"],inplace=True)
```

表示检查数据A列，如果有缺失值就删除那一行数据，就地删除。参数subset表示处理哪些特征数据。



### 165、Imputer

用于填充数据中的缺失值，默认是平均值填充



### 166、XGB

**n_estimators** 表示建模的次数，值太小会导致拟合不足，值太大又会导致过拟合。

**early_stopping_rounds**  会在验证分数停止提高时停止迭代，一般设置为5即可，模型在拟合过程中，可以设置 eval_set=[(x_val,y_val)] 表示用哪些数据来计算指标。

**learning_rate** 表示学习率

**gamma** 表示分裂结点所需的最小下降值，当gamma大于增益时，会对树进行裁剪。

**min_child_weight** 一个叶节点上的最小参数权重

**eval_metric** 表示评估标准，应该在初始化的时候就传入

**early_stopping_rounds** 表示训练结果不再提升就停止的轮数

相似度分数 会衡量结点的纯净程度；增益是分裂前后相似度分数的差值，决定是否分裂；叶子输出值用结点梯度和计算出来，以决定分裂完成后预测更新多少。





### 167、Hyperopt 

#### 1、基本方法

| 方法                               | 意思                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| `hp.quniform(label, low, high, q)` | 离散均匀分布，取值在 `[low, high]`，步长 q，比如 max_depth=3~18，每次取整数，生成的是浮点数 |
| `hp.uniform(label, low, high)`     | 连续均匀分布，取值在 `[low, high]`，可以是浮点数             |

这些方法返回的不是一个数，而是一个“搜索空间表达式”，类型是  Apply(Hyperopt的内部对象)



#### 2、返回值

Hyperopt 规定：目标函数必须返回一个字典，告诉它：

1. 当前超参数组合的 **损失值** (`loss`)（优化的目标必须是越小越好，因此使用 fmin优化，对于准确率，通常加一个负号。）
2. 当前尝试是否成功 (`status`)

```
return {
    'loss': <float>,      # 要最小化的目标值
    'status': STATUS_OK   # 状态标识
}
```



#### 3、解决问题的一般步骤

1. 首先是找到要调节的参数和调参范围
2. 设置求评估分数的函数，最后返回一个字典
3. 寻求最佳超参数

```python
trials = Trials()

best_hyperparams = fmin(fn = objective, # 目标函数
                        space = space, # 参数空间
                        algo = tpe.suggest,  # 搜索算法
                        max_evals = 100,   # 目标函数被调用的次数
                        trials = trials)   # 记录尝试结果
```

`tpe.suggest` → Tree-structured Parzen Estimator，贝叶斯优化，推荐使用

`rand.suggest` → 随机搜索

`anneal.suggest` → 模拟退火



### 168、特征工程

#### 1、MI（互信息）

衡量一个量的至三十再多大程度上减少对另一个量的不确定性的度量，指越大越好，0.5 ~ 1表示中等关系，1 ~ 2表示强关系，大于2表示完美预测。但是MI增长缓慢，就像对数一样。但是呢MI只看单个特征，忽略其他特征的关系。并且呢，MI知识帮你找到有价值特征，并不会直接帮助你训练，需要做特征处理才能使模型健壮。

在 sklearn 中，

```
from sklearn.feature_selection import mutual_info_regression

# 参数 discrete_features 会标记离散特征以便模型做单独处理
```

一方面，通过互信息，你可以看到一些特征是稀有特征，对大多数数据没有什么用，反而会扰乱整个数据集的训练，所以我们通常要**避免少量数据特有特征**对整个数据的影响。另一方面，MI对于交互特征并不敏感，因此，我们需要观察数据特征，寻找可能有关联的信息，可能它的MI较低，但并不意味着不重要。

#### 2、处理特征

你可以为新建数据特征，常见的有求和、是否大于某个值、将独特的字符串分割成多个特征、分组变换（求出不同类别具有的特点，可以是平均值、比例或者数目等）

对于不可比的数据，通常会进行缩放。对于可比的数据，需要视情况选择，因为该数据蕴含了直接的大小关系，如果缩放的话只能得到相对关系，丢失绝对关系。

#### 3、PCA（主成分分析）

把数据标准化之后，PCA实际上实在分析变量之间的相关性结构。如果不标准化，PCA则是在分析协方差结构。PCA会找出数据变化最大的方向，也就是能够很好区分这些数据点的“主方向”（PC1），此时第一主成分不再是单个特征，而是多个特征的某种线性组合。紧接着，PCA会找一个和主成分方向垂直的方向，解释剩余的变化。新特征是原始特征的线性组合，新特征称为数据的主成分，权重称为载荷。

PC1找到的是变化最大的方向，但是数据不止沿着该方向变化，所以就有了PC2、PC3等。PC2与PC1垂直，因为如果不垂直，二者就会捕捉一些相同的信息，造成资源浪费。变化最大的方向也就是方差最大的方向，高方差表示数据变化大，蕴含的信息多，重要；低方差表示数据不怎么变化，因此蕴含的信息也比较少，可以考虑舍弃。

PCA有两种主要的应用方法。第一种是用作描述性技术，因为PCA揭示了原始数据的变化方向，可以据此来设计优秀特征。第二种是直接将主成分作为特征使用，有降维、异常检测、去噪和去相关性，下面一一展开。首先是降维，PCA会找到高方差主成分，把冗余信息集中到少数成分中并丢弃。第二是异常检测，异常点在原始高维空间中可能不明显，在低方差主成分对应得方向上会偏离群体，在该方向上更容易被发现。第三是降噪，高方差主成分是主要信号，低方差主成分是背景噪声，通常我们会保留高方差分量，丢弃噪声。第四是去相关性，因为一些算法（如线性回归、KNN等）对高度相关特征敏感，而PCA输出得主成分彼此正交，无相关性，可以加速收敛。

#### 4、无监督学习

无监督学习可以作为特征发现技术。





### 169、布尔值

原来布尔值是可以相加的，因为布尔逻辑本质上是 0/1 运算，hh。



### 170、gt

pandas数据又gt方法用于比较值

```python
data.gt(x) #  返回大于x的布尔矩阵
```



### 171、跨行表达式

（）、[]、{ } 可以让一行数据跨行书写，保证代码清晰明了



### 172、str  访问器

```python
data.str.lower()
data.str.split(".",n=1expand=True)
```

str访问器可以对pandas数据进行字符串操作，其中expand参数可以让数据分割成两列数据，而不再是列表，参数n表示只再第一处进行拆分。



### 172、map和apply

map针对series数据，即单列数据处理，而apply既可以处理单列数据又可以处理表格数据。



### 173、transform

对于pandas数据，有transform方法，可以对每个分组湖泊这每列应用函数，然后返回一个和原对象相同的结果。有max`、`min`、`median`、`var`、`std` 和 `count可以选择



### 174、mul

data.mul(data2 ,axis=0)表示按行逐元素相乘，虽然axis违背了直觉。



### 175、pd.get_dummies

该函数用于独热编码，参数prefix=" "指定字符串，可以在生成的列名前面加上他。



### 176、hue参数

在sns绘图里面，该参数可以设置点的颜色

| 场景             | 示例                                | 说明                                                       |
| ---------------- | ----------------------------------- | ---------------------------------------------------------- |
| **分类变量分组** | `hue="species"`                     | 最常见的用法，不同类别不同颜色                             |
| **二分类目标**   | `hue="target"`                      | 正负样本用不同颜色，看分布差异                             |
| **聚类结果**     | `hue="Cluster"`                     | 不同聚类结果不同颜色（搭配 `astype("category")` 效果更好） |
| **分组对比**     | `hue="gender"`                      | 男/女用不同颜色看趋势                                      |
| **数值映射颜色** | `hue="value"` + `palette="viridis"` | 用渐变色表示大小，常见于散点图或热力图                     |



### 177、pop方法

pands数据可以使用pop方法删除某一列数据，同时赋值给另一个数据

```
y=x.pop("salesprice")
```



### 178、cpoy方法

```python
data.copy(deep=True)
```

默认是深拷贝，也就是说拷贝出的数据与原数据不共享底层。



### 179、join （行索引对齐）

```python
data.join(x)
```

按行索引对齐，把x直接添加到data右侧



### 180、nunique  和  unique

pd  数据有这两种方法。nunique可以返回各个特征得类别数，unique则会返回各个特征得独特标签数值或者说名字。value_counts则是查看一列数据中的类别和相应的样本数量，要注意区分。



### 181、smaple方法

pd数据有sample方法，可以随机抽取行，参数frac表示抽取数据得比例。

```python
X_encode = X.sample(frac=0.25)
```



### 182、 M-Estimate 编码

```python
encoder = MEstimateEncoder(cols=["Zipcode", "Neighborhood"], m=5.0)

```

$$
\text{EncodedValue}_c = \frac{n_c \cdot \text{mean}_c + m \cdot \text{global mean}}{n_c + m}
$$

该编码方式会对类别列中的每个类被，计算类别在目标变量上的平均值，有参数m，控制小类别的影响。m越大，值越接近全局平均，用于减小小类别的影响，m越小，值越接近类别均值，其编码值是连续值，这是需要记住的。可以起到  “降维” 的效果。

该编码方式在特征类别数大，不好直接独热编码，同时也不想引入顺序关系（但是，对树模型而言是没有顺序这个概念的）。

需要注意的是，编码时，你需要单独划分出一部分数据专门用于编码，防止产生数据泄露。

















## 2.项目心得

### 1. `titanic` 竞赛

1. 随机森林在分类竞赛中可以优先考虑。

2. 处理数据时，可以先通过这些手段来评估数据，并进行缺失值显示、处理，字符串数据转换（分有序和无序）

   ```
   data.info()
   data.describe()
   data[target].value_counts()
   
   ```

3. 可以通过交叉验证损失，并得到模型

4. 对于训练集，先进行数据划分，以便之后进行模型评估

5. 深度学习不是唯一手段，也可以结合sklearn处理。

6. 本次竞赛中，学会了利用sklearn处理数据的手段，比如k折交叉验证，数据划分，数据按列分别处理，基线模型。



### 2.`house-price`竞赛

1. 如果目标数值大，且跨度大，可以考虑使用对数损失。同时你需要对`target` 先进行对数变化，后进行标准化，以缩小范围，便于模型拟合。之后再反标准化和对数化。

   ```python
   def log_rmse(net, features, labels):
       """
       计算对数均方根误差（log RMSE），返回 PyTorch 张量。
       
       参数：
           net: 神经网络模型
           features: 输入特征张量，形状 [batch_size, n_features]
           labels: 标准化的目标张量，形状 [batch_size, 1]
       
       返回：
           torch.Tensor: log RMSE 损失
       """
       pred = net(features)
       mse = ((pred - labels) ** 2).mean()  # 标准化的 MSE
       return torch.sqrt(mse)  # 返回张量，保留计算图
   ```

2. 可以把训练集和测试集先合并起来，一并处理，之后再分开。

3. `numeric_data = data.dtypes[data.dtypes != "object"].index`可以快速提取数值特征。

4. pandas数据转换为张量时，先data.values，把数据转换成np数组，之后再转换成张量。

5. 加载数据方法，先Dataset，后进行Dataloader

   ```python
   def dataloader(data, batch_size, is_train):
       dataset = TensorDataset(*data)
       return DataLoader(dataset, batch_size, shuffle=is_train)
   ```

   

6.预测时，可以

```python
net.eval()            # 切换到评估模式，Dropout & BatchNorm 等失活，屏蔽丢弃神经元的影响
with torch.no_grad(): # 禁止梯度计算
...
```

### 3、` bank `竞赛

1. 本次竞赛中，我对 torch 的基本技巧比较熟练了，并且能够使用sklearn的技巧对训练集进行划分，并且进行评估。并且我能够先划分数据集来评估模型的性能，之后再合并数据集再次训练，并进行正则化惩罚，达到了不错的效果。



### 4、`CIFAR-10`竞赛

1.  本次竞赛中，我遇到了不少的报错和挑战，这是我第一次基本独立完成的图像分类竞赛。

2. 对数据做变换时，要记着，不仅要对验证集还要对测试集做同样的变换。

3.  本次比赛我还使用了归一化处理，如果图象数据来自imagenet或者cifar10，你需要记住一些参数，如下。否则，需要自己计算相应的参数，并进行处理，如下。但是这样的话均值和方差都是tensor类型，那么我们就需要把他们转换成列表格式。

   ```python
   <  imagenet  >
   mean = [0.4914, 0.4822, 0.4465]
   std  = [0.2023, 0.1994, 0.2010]
   
   <  cifar  >
   mean = [0.4914, 0.4822, 0.4465]
   std  = [0.2023, 0.1994, 0.2010]
   
   # 假设 train_data shape = (N, C, H, W)
   mean = train_data.mean(dim=[0,2,3])
   std  = train_data.std(dim=[0,2,3])
   transforms.Normalize(mean=mean.tolist(), std=std.tolist())
   ```

4. [在读取图像数据时，参见88](# 88、listdir（）)

5. 读取数据，无论什么数据时，最好将这一过程单独成单元格以及数据名称保证只用一次，这样的话，数据就不会被更改，也不用反复重新读取了。有条件的话，加上tdqm，可以可视化进度。

6. 关于标签编码问题，如果标签只有一列的话，直接使用LabelEncoder更好，因为他会生成一个1d 的ndarray的数据，属性classes_记录映射关系。而OrdinarEncoder 虽然也能起到同样的作用，但是它返回的ndarray是多维的，最好还要去维，比较麻烦，还是适合处理多列数据，属性categories _ 记录映射关系。相同的是，二者都是从0开始编码。

7. Dataset是一个数据集基类，不是一个有具体功能的类。当你要对数据进行处理比如变换时，你就可以创建一个继承Dataset的新类。

8. 关于设备，最好在一开始就指定，这样子清晰明了

   ```python
   device=torch.device("cuda")
   ```

9. train_test_split函数也可以接收张量，这点值得关注。

10. 学会使用学习率调度去辅助调节模型性能，[参见](# 89、学习率调度器（精简版）)。

11. 损失的reduction参数可以是"mean"，当计算总损失时，直接

    ```python
    loss_epoch+=loss.item()*len(y)
    ```

12. 绘制曲线时，可以直接绘制训练集损失、训练集准确率，测试集准确
13. 最后，本次竞赛涉及的知识整理范围如下：（74~90）。



### 5、`Kannada-MSIST` 竞赛

1. 一般情况下，将数据保存在cpu上，在训练循环里面再调整device，或者说在用到的时候再转移设备，这样就不会混淆了。
2. 在计算训练精度时，不要一下把数据都放到gpu上训练，会造成内存不足，也是应该分批次计算。计算验证损失时也是如此。同时你还需要把模型设置成评估模式，在函数末尾设置为训练模式。
3. 状态依旧糟糕，想放弃。



### 6、`K-Means Clustering for Heart Disease Analysis` 竞赛

1. 对于pandas数据，直接for循环遍历，得到的是列名 for col in data。
2. 本次竞赛，我学习并练习了聚类知识，让我印象深刻的是数据处理，之后就是肘部法（越小越好），以及silouette_score（越大越好），之后就没什么好说的了。
3. 加油



### 7、`bank`竞赛（回顾，采用经典机器学习）

#### 1、EDA 

1. 对于数值特征而言。当数据偏斜（数据有个小尾巴），即数据分布不对成不均匀时，直接训练的话，像逻辑回归等线性模型对极端值敏感，拉低训练效果；像神经网络模型，可能会在极大值处梯度爆炸或者极小值梯度消失；对于树模型，倒是不影响，毕竟他不看具体数值，只看分布。常用的手段有 pd.sqrt、pd.cbrt（立方根）、np.log1p等。

```python
# 对数变换，np.sign表示返回各个元素符号
 df['balance_log'] = (np.sign(df['balance']) * np.log1p(np.abs(df['balance']))).astype('float32')
    
# sin/cos 变换
df['week_sin'] = np.sin(2*np.pi*df['week_num']/7)
df['week_cos'] = np.cos(2*np.pi*df['week_num']/7)

#分箱操作，你可以先进行数据变换，之后进行分箱
df['duration_bin_20'] = pd.qcut(
    df['duration'],      # 要分箱的连续特征
    q=20,               # 分成20个箱（每箱大约5%的样本）
    labels=False,       # 用整数标签表示箱号，而不是返回区间对象
    duplicates='drop'   # 如果分位点有重复值，丢掉重复的箱，避免报错
)
```



2. 对于数值特征而言，有时，我们也通过多种变换构建新的特征以帮助模型训练，就比如主动sqrt、sin\cos变换、分箱操作

3. 对于非数值列而言，什么时候使用 OneHotEncoder ，什么时候使用  OrdinalEncoder  ，以及什么时候 对数据进行sin，cos处理呢。我的看法是：当数据存在明显的顺序关系但不是周期性时，就比如本科->硕士->博士，这时候使用 OrdinalEncoder 或者 LabelEncoder 就挺好的。当数据类互不相关时，直接用One Hot Encoder是最佳选择。当你发现数据存在周期性时，那就使用 sin、cos处理数据，但要注意你需要先把非线性值映射到线性值，之后再用该方法处理。

#### 2、超参数搜索

当数据量大时，采用直方图梯度提升是个不错的方法，效率高。并且可以使用随机搜索的方法，一开始使用数据的50%训练，之后不断提高比例，70%左右时，可以停下。之后再次运行一下，看看超参数规律是不是偶然。之后，使用全部数据拟合，但不再搜索超参数，并根据之前的规律适当调整超参数，与此同时不断用验证集测试。

最后，使用train和val数据中的0.9训练，0.1作为参考，得到的模型直接测试。或者使用全部数据，用kaggle结果观察效果。

#### 3、TE（类别编码）

见到了一个新的变换器FunctionTransformer（function），可以对数据直接进行变换。

#### 4、数据显示

1. plt.xticks(rotation=45)调整x轴标签角度，常在类被较多时使用。
2.  percentage_data=(train_data.groupby(col["y"].value_counts(normalize=True).rename("percentage").reset_index())可以按类别显示，后面再跟上barplot显示，即条形图显示。

3. histplot，即直方图显示，显示连续数据分布。

### 8、`cat and dog ` 分类竞赛

一坨

